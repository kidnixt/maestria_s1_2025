#  Gu铆a de Estudio Detallada: Aproximaci贸n de Valor y MCTS 

---

### **Diapositiva 1: Portada**

* **Resumen Estricto:**
    * **Curso:** Sistemas Multiagente.
    * **Tema:** Juegos Alternados: Funciones de evaluaci贸n - Aproximaci贸n del valor.
    * **Autor:** Sergio Yovine, Universidad ORT Uruguay.
    * **Fecha:** 7 de mayo de 2025.

---

### **Diapositiva 2: Minimax con Funciones de Evaluaci贸n (Repaso)**

* **Resumen Estricto:**
    * Se repasa la f贸rmula recursiva del algoritmo Minimax cuando se utiliza una **funci贸n de evaluaci贸n ($Eval(s)$)** y se limita la b煤squeda a una **profundidad m谩xima ($d$)**.
    * Se enfatiza que el valor calculado, $\hat{V}$, es una **aproximaci贸n** del verdadero valor Minimax.

* **Ecuaciones de la Diapositiva:**
    * F贸rmula de Minimax aproximado:
        $$ \hat{V}_{\pi_{minimax},\pi_{min}}(s,d)= \begin{cases} Utilidad(s) & \text{si EsFinal(s)} \\ Eval(s) & \text{si } d=0 \\ max_{a\in Acciones(s)}\hat{V}_{\pi_{minimax},\pi_{min}}(Suc(s,a),d-1) & \text{si Jugador(s) = Agente} \\ min_{a\in Acciones(s)}\hat{V}_{\pi_{minimax},\pi_{min}}(Suc(s,a),d-1) & \text{si Jugador(s) = Oponente} \end{cases} $$
    * Relaci贸n con el valor real:
        $$ \hat{V}_{\pi_{minimax},\pi_{min}} \text{ es una aproximaci贸n de } V_{\pi_{minimax},\pi_{min}} $$

* **Flashcards :**
    * **Pregunta:** 驴Por qu茅 el valor calculado con una funci贸n de evaluaci贸n se considera una "aproximaci贸n"?
        > **Respuesta:** Porque no explora el juego hasta el final. La funci贸n `Eval(s)` es una "conjetura" o "estimaci贸n" de qu茅 tan buena es una posici贸n, no su valor real y definitivo.

---

### **Diapositiva 3: Funciones de Evaluaci贸n con Estrategias Mixtas**

* **Resumen Estricto:**
    * Se propone una forma de definir una funci贸n de evaluaci贸n: $Eval(s)$ puede ser el valor esperado del juego desde el estado `s`, asumiendo que a partir de ese punto ambos jugadores siguen unas **estrategias mixtas** fijas ($\pi_a, \pi_o$).
    * Se compara el resultado de Minimax (valor 1) con el resultado de un oponente que juega al azar (valor esperado 2.3), mostrando que la evaluaci贸n cambia seg煤n la pol铆tica del oponente.
    * Se advierte que calcular este valor esperado de forma exacta es computacionalmente "Caro!".

* **Ecuaciones de la Diapositiva:**
    * Definici贸n de la funci贸n de evaluaci贸n:
        $$ Eval(s)=V_{\pi_{a},\pi_{o}}(s) $$
    * F贸rmula recursiva para calcular dicho valor:
        $$ V_{\pi_{a},\pi_{o}}(s)= \begin{cases} Utilidad(s) & \text{si EsFinal(s)} \\ \sum_{a\in Acciones(s)}\pi_{a}(s,a)V_{\pi_{a},\pi_{o}}(Suc(s,a)) & \text{si Jugador(s) = Agente} \\ \sum_{a\in Acciones(s)}\pi_{o}(s,a)V_{\pi_{a},\pi_{o}}(Suc(s,a)) & \text{si Jugador(s) = Oponente} \end{cases} $$

* **Flashcards :**
    * **Pregunta:** 驴Cu谩l es una forma de crear una funci贸n de evaluaci贸n que no sea simplemente contar piezas?
        > **Respuesta:** Asumir que desde la posici贸n actual, ambos jugadores jugar谩n con una pol铆tica fija (por ejemplo, aleatoria) y calcular el valor esperado del resultado.

---

### **Diapositiva 4: Funciones de Evaluaci贸n: Monte Carlo**

* **Resumen Estricto:**
    * Introduce el m茅todo de **Monte Carlo** para *aproximar* la funci贸n de evaluaci贸n sin tener que calcularla de forma exacta.
    * **Idea:** Para evaluar un estado `s`, se simulan `m` episodios completos (partidas) desde `s`. En estas simulaciones, los jugadores siguen unas pol铆ticas fijas.
    * La estimaci贸n del valor de `s` es simplemente el **promedio** de los resultados (utilidades) obtenidos en esas `m` simulaciones.

* **Ecuaciones de la Diapositiva:**
    * Estimaci贸n del valor:
        $$ Eval(s)=\hat{V}_{\pi_{a},\pi_{o}}(s) $$
    * Valor del j-茅simo episodio simulado:
        $$ V_{\pi_{a},\pi_{o}}^{j}(s) $$
    * F贸rmula de la media de Monte Carlo:
        $$ Eval(s)=\hat{V}_{\pi_{a},\pi_{o}}(s)=\frac{1}{m}\sum_{j=1}^{m}V_{\pi_{a},\pi_{o}}^{j}(s) $$

* **Ejemplo Did谩ctico:**
    * Quieres saber si una posici贸n de Ta-Te-Ti es buena. En lugar de pensar, simplemente juegas 100 partidas hasta el final desde esa posici贸n, haciendo jugadas al azar para ambos jugadores. Si ganas 70, empatas 20 y pierdes 10, tu estimaci贸n Monte Carlo del valor de esa posici贸n es $(70 \times 1 + 20 \times 0 + 10 \times -1) / 100 = 0.6$.

* **Flashcards :**
    * **Pregunta:** 驴Cu谩l es la idea principal del m茅todo de Monte Carlo para evaluar un estado?
        > **Respuesta:** Simular muchas partidas al azar desde ese estado y promediar los resultados para estimar su valor.

---

### **Diapositiva 5: Funciones de Evaluaci贸n: Rollout**

* **Resumen Estricto:**
    * **Rollout** es el t茅rmino que se le da al proceso de simular un episodio desde un estado `s` hasta un estado terminal.
    * El m茅todo consiste en repetir el rollout `m` veces y promediar los resultados.
    * Se se帽alan dos problemas: puede ser lento si los episodios son largos, y para solucionar esto se puede acotar la longitud del rollout y usar otra funci贸n de evaluaci贸n m谩s simple al final.

* **Ecuaciones de la Diapositiva:**
    * Actualizaci贸n del valor acumulado en un bucle:
        $$ V_{rollout}(s)=V_{rollout}(s)+V_{\pi_{a},\pi_{o}}^{j}(s) $$
    * Valor final:
        $$ V_{rollout}(s)/m $$

* **Ejemplo Did谩ctico:**
    * "Rollout" es la simulaci贸n en s铆. Cuando un programa como AlphaGo est谩 "pensando", una de las cosas que hace es realizar miles de estos "rollouts" para cada jugada posible. Un rollout es una partida r谩pida y "tonta" (usando una pol铆tica simple) para tener una idea r谩pida de si la jugada lleva a una victoria o a una derrota.

* **Flashcards :**
    * **Pregunta:** 驴Qu茅 es un "rollout"?
        > **Respuesta:** Es la simulaci贸n de un 煤nico episodio o trayectoria desde un estado hasta el final del juego, generalmente usando una pol铆tica simple y r谩pida.

---

### **Diapositiva 6 y 7: Monte Carlo Tree Search (MCTS)**

* **Resumen Estricto:**
    * Se introduce **Monte Carlo Tree Search (MCTS)**, un algoritmo que combina la b煤squeda en 谩rboles con las simulaciones de Monte Carlo.
    * MCTS construye un 谩rbol de b煤squeda de forma asim茅trica, centr谩ndose en las zonas m谩s prometedoras.
    * Para decidir qu茅 rama explorar, utiliza una "pol铆tica del 谩rbol", como **Upper-Confidence Bound (UCB)**, que equilibra la **explotaci贸n** (elegir la jugada que parece mejor hasta ahora) y la **exploraci贸n** (probar jugadas menos exploradas por si resultan ser buenas).
    * Dentro del 谩rbol se usa UCB, y al llegar a una hoja del 谩rbol construido, se hace un rollout para estimar su valor.

* **Ecuaciones de la Diapositiva:**
    * F贸rmula de Upper-Confidence Bound (UCB) para un nodo `n`:
        $$ UCB(n)=\underbrace{X_{n,m}}_{\text{Explotaci贸n}} + \underbrace{C\sqrt{\frac{log~m}{T_{n}(m)}}}_{\text{Exploraci贸n (Incertidumbre)}} $$
    * **$X_{n,m}$**: Valor promedio del nodo `n` (su "tasa de victorias").
    * **$m$**: N煤mero de visitas al nodo padre de `n`.
    * **$T_n(m)$**: N煤mero de visitas al propio nodo `n`.
    * **$C$**: Constante que ajusta el nivel de exploraci贸n.

* **Flashcards :**
    * **Pregunta:** 驴Qu茅 dos conceptos equilibra la f贸rmula UCB en MCTS?
        > **Respuesta:** Equilibra la **explotaci贸n** (preferir los movimientos que han demostrado ser buenos) y la **exploraci贸n** (probar movimientos sobre los que se tiene m谩s incertidumbre).
    * **Pregunta:** 驴Cu谩l es la diferencia principal entre MCTS y un simple Monte Carlo?
        > **Respuesta:** El Monte Carlo simple simula desde la ra铆z y ya. MCTS construye un 谩rbol, guardando estad铆sticas en cada nodo y usando esa informaci贸n para guiar las futuras simulaciones hacia las partes m谩s interesantes del juego.

---

### **Diapositiva 8 y 9: Los 4 Pasos de MCTS**

* **Resumen Estricto:**
    * Se describen los cuatro pasos que se repiten en cada iteraci贸n de MCTS:
    1.  **Selecci贸n (Selection):** Se parte de la ra铆z y se desciende por el 谩rbol existente usando la pol铆tica UCB para elegir el camino m谩s prometedor.
    2.  **Expansi贸n (Expansion):** Al llegar a un nodo hoja del 谩rbol actual, se expande a帽adiendo uno o m谩s nodos hijos nuevos.
    3.  **Simulaci贸n (Simulation):** Desde uno de estos nuevos nodos, se realiza un rollout (simulaci贸n r谩pida y aleatoria) hasta el final del juego para obtener un resultado (victoria/derrota).
    4.  **Propagaci贸n hacia arriba (Backup):** El resultado de la simulaci贸n se propaga hacia atr谩s por el camino seleccionado, actualizando las estad铆sticas (visitas y victorias) de todos los nodos en ese camino.

* **Ejemplo Did谩ctico:**
    * Imagina que MCTS es un explorador de cuevas.
    1.  **Selecci贸n:** Va por los t煤neles que ya conoce y que parecen m谩s prometedores.
    2.  **Expansi贸n:** Llega a un punto sin explorar y abre un nuevo t煤nel.
    3.  **Simulaci贸n:** Env铆a un dron r谩pido por el nuevo t煤nel para ver si lleva a un tesoro o a un callej贸n sin salida.
    4.  **Propagaci贸n:** Vuelve sobre sus pasos y va marcando en su mapa: "este camino llev贸 a algo bueno" o "este camino llev贸 a algo malo".
    * Repite esto miles de veces para construir un mapa muy detallado de la cueva.

* **Flashcards :**
    * **Pregunta:** Nombra los cuatro pasos del bucle de MCTS.
        > **Respuesta:** Selecci贸n, Expansi贸n, Simulaci贸n y Propagaci贸n (Backup).

---

### **Diapositiva 10: Implementaci贸n de MCTS**

* **Resumen Estricto:**
    * Se presenta un pseudo-c贸digo de alto nivel para el algoritmo MCTS.
    * Se crea un nodo ra铆z. Luego, se itera un n煤mero determinado de veces realizando los 4 pasos (Select, Expand, Rollout, Backprop).
    * Al final, la mejor acci贸n se elige desde el nodo ra铆z, bas谩ndose en las estad铆sticas acumuladas (normalmente, la acci贸n que lleva al hijo m谩s visitado).

* **Ecuaciones de la Diapositiva (Pseudo-c贸digo):**
    ```
    function mcts(G, s0, t, r):
      Crear nodo ra铆z n0 con estado s0
      for i = 1 to t do:
        // 1. Selecci贸n
        n <- Select(n0)
        // 2. Expansi贸n
        n' <- Expand(G, n)
        // 3. Simulaci贸n
        v <- Rollout(G, n', r)
        // 4. Propagaci贸n
        Backprop(n', v)
      // Decisi贸n final
      return BestAction(n0)
    ```

* **Flashcards :**
    * **Pregunta:** Despu茅s de miles de iteraciones de MCTS, 驴c贸mo se elige finalmente la mejor jugada?
        > **Respuesta:** Generalmente se elige la acci贸n que lleva al nodo hijo m谩s visitado desde la ra铆z, ya que esto indica que el algoritmo ha determinado que es la opci贸n m谩s robusta y prometedora.

---

### **Diapositiva 11: Bibliograf铆a**

* **Resumen Estricto:**
    * Se recomienda el famoso libro de "Reinforcement Learning" de Sutton y Barto, y un paper de referencia sobre MCTS.