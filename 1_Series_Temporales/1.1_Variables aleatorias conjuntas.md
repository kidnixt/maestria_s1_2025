## Variables aleatorias conjuntas
Cuando definimos espacio muestral, comentamos que $\Omega$ tiene "todo lo que puede pasar" en nuestro experimento. Ahora, podemos tener varias variables aleatorias que surjan como medidas de realizar el *mismo experimento*. En este caso las v.a. están definidas sobre un mismo espacio de probabilidad y se denominan v.a. conjuntas.

**Ejemplo:**
- Experimento: extraer una persona al azar de la población
- Variables: $X$ = altura, $Y$ = peso de la persona.

Podría también tomarse $Z$ = índice de masa corporal $= \frac{Y}{X^2} = G(X,Y)$, una tercera v.a. que es función de las anteriores.

### Función de densidad conjunta
Cuando las variables se comportan de manera conjunta, la noción de densidad es más complicada. Es una función de todas las posibles combinaciones de valores ($x_1, ..., X_n$):

> **Definición:** Una conjunto de variables aleatorias $(X_1,\ldots,X_n)$ tiene *densidad conjunta* $f(x_1,\ldots,x_n)$ si y solo si para toda región $R\subset \mathbb{R}^n$:
>$$ P(X \in R) = \int\cdots\int_R f(x_1,\ldots,x_n)dx_1\ldots dx_n$$

La función $f$ de densidad me indica como se reparte la masa en el espacio ($x_1, ..., x_n$). Normalmente esto lo visualizamos haciendo "nubes de puntos" con muestras de la variable conjunta.

> En general no vamos a calcular la función de densidad conjunta.

### Densidad marginal
Si dos variables (o más) tienen densidad conjunta $f(x,y)$, se pueden obtener sus densidades marginales, es decir, la de cada una por separado. Se hace integrando sobre todos los posibles valores de la(s) otra(s) variables.

Por ejemplo, si $(X,Y) \thicksim f(x,y)$ entonces:
$$
f_X(x) = \int_{-\infty}^{\infty} f(x,y)dy \\
$$
$$
f_Y(y) = \int_{-\infty}^\infty f(x,y) dx
$$

#### Ejemplo: variables uniformes independientes
Supongamos que elegimos puntos al azar completamente en el cuadrad $[0,1] \times [0,1]$. Esto corresponde a sortear de manera uniforme sus coordenadas.

```python
## sorteo uniforme las coordenadas
X=np.random.uniform(size=100) #500 puntos entre 0 y 1 uniformes
Y=np.random.uniform(size=100) #500 puntos más para la otra coordenada

plt.figure(figsize=(5,5))
plt.scatter(X,Y)
plt.axis("equal");  #dibuja ambas componentes en la misma escala
```
![[6b7768f8e6023ea22f73826f7bf380e756cc6852a6d11a4e55526c17b9073000.png]]

**Histogramas:**
```python
plt.subplot(1,2,1)
plt.hist(X)
plt.title(r"Histograma de $X$")
plt.subplot(1,2,2)
plt.hist(Y)
plt.title(r"Histograma de $Y$");
```
![[6ebec56cf76e342d267ef8ec04c9d06838399cfd29f54589fa9d5bfadbad5a49.png]]
Cada histograma representa la densidad marginal de cada variable. 

## Media y Varianza
La media y varianza de variables se definen como antes, usando las marginales:
$$
\mu_X = E[X] = \int x f(x)dx = \int\int x f(x,y) dxdy
$$
$$
\mu_Y = E[Y] = \int y fy(y) dy = \int\int y f(x,y)dxdy
$$
$$
\sigma^2_X = E[(X-\mu_X)^2] \quad \\ \sigma^2_Y = E[(Y, \mu_Y)^2]
$$

Y en general para cualquier función $g: \mathbb{R}^2 \rightarrow \mathbb{R}$ que dependa de ambas variables:
$$
E[g(X,Y)] = \int\int g(x,y)f(x,y)dxdy
$$

## Independencia de variables aleatorias

Dos variables son independientes si y solo si conocer el valor de una no afecta la distribución de la otra variable. Es decir, saber el resultado de una no cambia mi percepción sobre la otra, sigue teniendo la misma distribución que antes.

Esto se cumple si y solo si vale la siguiente regla producto:
$$
f(x,y) = f_X(x)f_Y(y)
$$
En ese caso, $P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$ para cualquier suceso $A, B$ y en particular: 
$$
P(Y \in B | X = x) = P(Y \in B)
$$
que es lo que pide la definición.

### Ejemplo: variables no independientes
En el [[#Ejemplo variables uniformes independientes]] precisamente sorteamos $X$ e $Y$ de manera independientes, por lo cual saber la coordenada $X$ no cambia mi percepción sobre la coordenada $Y$ (sigue siendo uniforme en $[0,1]$).

Tomemos una transformación de los puntos anteriores:
$$
U = X+Y \quad \\ V=X-Y
$$
En este caso, hay *información cruzada* entre las variables.

```python
## sorteo uniforme las coordenadas
X=np.random.uniform(size=500) #500 puntos entre 0 y 1 uniformes
Y=np.random.uniform(size=500) #500 puntos más para la otra coordenada

U=X+Y
V=X-Y

plt.figure(figsize=(5,5))
plt.scatter(U,V)
plt.axis("equal");  #dibuja ambas componentes en la misma escala
```
![[Untitled.png]]

**Histogramas:**
```python
plt.subplot(1,2,1)
plt.hist(U)
plt.title(r"Histograma de $U$")
plt.subplot(1,2,2)
plt.hist(V)
plt.title(r"Histograma de $V$");
```
![[Untitled 1.png]]

**¿Por qué no son independientes?
- Las variables $U$ y $V$ así construidas tienen una distribución marginal en forma de triangulo.
- La suma $U$ va entre $0$ y $2$ con valor más probable $1$.
- La resta $V$ va entre $-1$ y $1$ con valor más probable $0$.

Además **no son independientes:** observamos tanto $X$ como $Y$ participan en la construcción de $U$ y $V$ por lo que es de esperar que esto ocurra.

En particular si sabemos que $U$ es grande (la suma esta cerca de $2$) entonces tanto $X$ como $Y$ tienen que estar cerca de $1$, por lo que su diferencia debe estar cerca de $0$.

De este modo hay información cruzada.

## Covarianza: una medida de dependencia
Dadas dos variables aleatorias $X$ e $Y$ conjuntas, definimos la *covarianza* entre ellas como:
$$
\mathrm{Cov}(X,Y) = E[(X-\mu_X)(Y-\mu_Y)]
$$
Es decir, cuánto varía en promedio el producto de los desvíos respecto a la media.

**Idea:**
- Si dos variables están asociadas de modo que valores altos de una (respecto a su media) producen valores altos de la otra (respecto a su media), tendrán covarianza alta y positiva.
- Si dos variables están asociadas negativamente, es decir, valores altos de una (respecto a su media) produce valores bajos de la otra (respecto a su media), tendrán covarianza alta y negativa.
- Si dos variables son independientes entonces su covarianza será 0.

**Nota:** No necesariamente se cumple que si la covarianza es 0 entonces las variables son independientes, sin embargo nosotros vamos a considerar que esto se cumple siempre.

- $\mathrm{Cov}(x,y)^2 \leq \sigma_x^2 \times \sigma_y^2 \iff \frac{\mathrm{Cov}(x,y)}{\sigma_x\sigma_y} \leq 1$
- $\mathrm{Cov}(x,y) = \mathrm{Cov}(y,x)$
- $\mathrm{Cov}(x,x) = Var(x)$

Si tenemos $(X_1, ..., X_n)$ conjuntas, a la matriz $\Sigma$ de todas las covarianzas entre ellas se le llama *matriz de covarianza*. En la diagonal tenemos $\mathrm{Cov}(X_i, X_i) = \mathrm{Var}(X_i)$. Y es una matriz **simétrica**.

**Propiedad de la covarianza (combinaciones lineales):**
Si $U = \sum_{j=1}^m a_j X_j$ y $V = \sum_{k=1}^n b_k Y_k$ son v.a. que resultan de combinar linealmente $\{X_j:j=1,\ldots,m\}$ y $\{Y_k:k=1\ldots,n\}$ entonces:

$$\textrm{Cov}(U,V) = \sum_{j=1}^m \sum_{k=1}^n a_jb_k \textrm{Cov}(X_j,Y_k).$$

Observar que lo anterior puede ponerse como $\textrm{Cov}(U,V) = a^T \Sigma b$ siendo $a$ y $b$ vectores columna y $\Sigma$ una matriz $m\times n$ cuyas entradas son las covarianzas.

En particular, si $X$ es un vector aleatorio con matriz de covarianzas $\Sigma$, y $U=a^T X$, $\textrm{Var}(U) = a^T \Sigma a$ (interpretando $a$ como vector columna).

**Ejemplo:**
Si quiero calcular $\mathrm{Cov}(X+Y, X-Y)$ tengo que hacer:
$$\begin{aligned}

\textrm{Cov}(X+Y,X-Y) &= \textrm{Cov}(X,X) - \textrm{Cov}(X,Y) + \textrm{Cov}(Y,X) - \textrm{Cov}(Y,Y) \\

                      &= \textrm{Var}(X) - \textrm{Var}(Y)

\end{aligned}$$
en este caso.

## Coeficiente de correlación
La covarianza tienen las mismas unidades que la varianza, y puede ser más grande o más chica dependiendo de la variabilidad intrínseca de $X$ e $Y$. Es decir, que la varianza de $X$ e $Y$ nos "ensucian" el valor de covarianza, y podría darnos que la covarianza es alta cuando en realidad es solo que la varianza de las variables es alta. Es por eso que se define el *coeficiente de correlación:*

$$\rho(X,Y) = \frac{\mathrm{Cov}(X,Y)}{\sqrt{\mathrm{Var}(X)}\sqrt{\mathrm{Var}(Y)}}.$$
Este tiene las siguientes propiedades:
* $-1\leqslant \rho(X,Y)\leqslant 1$.
* Si $X,Y$ independientes, $\rho(X,Y)=0$.
* $\rho(X,Y) = 1$ si y solo si $Y = aX+b$ para cierto $a>0$ y $b\in \mathbb{R}$.
* $\rho(X,Y) = -1$ si y solo si $Y = aX+b$ para cierto $a<0$ y $b\in \mathbb{R}$.

### Ejemplo: vectores aleatorios Gaussianos.
Un vector aleatorio Gaussiano $X = (X_1,...,X_n)$ es la generalización de la distribución normal a múltiples dimensiones. Está caracterizado por:
- Un vector de medias $\mu$ que dice el centro de cada una de las coordenadas.
- Una matriz de covarianzas $\Sigma$ que tiene:
	- En la diagonal, las varianzas de cada componente
	- Fuera de la diagonal, la covarianza de cada para de coordenadas (por lo tanto es simétrica)

> En `numpy` podemos sortear vectores gaussianos usando la función `np.random.multivariate_normal()`.

**Ejemplo**
Tomemos dos variables con $\mu=(0,0)$ (centradas) y con matriz de covarianzas:

$$\Sigma = \begin{pmatrix} 9 & 0 \\ 0 & 1 \end{pmatrix}$$
Es decir, la primera componente tiene más varianza que la segunda, y su covarianza es $0$.

```python
mu = np.array([0,0])
Sigma = np.matrix([[9,0],[0,1]])

X = np.random.multivariate_normal(mean=mu, cov=Sigma, size=500)

plt.figure(figsize=(5,5))
plt.scatter(X[:,0], X[:,1])
plt.axis("equal");
```
![[13c09f325348f8e620218983e234a4fe868f3537ac12be145ed4b05647c8afe5.png]]
```python
plt.subplot(1,2,1)
plt.hist(X[:,0])
plt.title(r"Histograma de $X_1$")
plt.subplot(1,2,2)
plt.hist(X[:,1])
plt.title(r"Histograma de $X_2$");
```
![[81e05af310e0c7eaa428cbe0e9c584582f5473cae045951fa937d162877fe074.png]]
```python
print(f"Media de X_1 = {np.mean(X[:,0])}")
print(f"Media de X_2 = {np.mean(X[:,1])}")
print(f"Varianza de X_1 = {np.var(X[:,0])}")
print(f"Varianza de X_2 = {np.var(X[:,1])}")

print(f"La Covarianza es:")
print(np.cov(X[:,0], X[:,1]))
print(f"La correlación es es:")
print(np.corrcoef(X[:,0], X[:,1]))
```

```
Media de X_1 = -0.20120016375895258
Media de X_2 = -0.018832990336931473
Varianza de X_1 = 9.397103002967055
Varianza de X_2 = 0.9312831950391468
La Covarianza es:
[[ 9.41593487 -0.09330451]
 [-0.09330451  0.93314949]]
La correlación es es:
[[ 1.         -0.03147714]
 [-0.03147714  1.        ]]
```

**Ejemplo:**
Tomemos dos variables con $\mu=(0,0)$ (centradas) y con matriz de covarianzas:

$$\Sigma = \begin{pmatrix} 3 & -2 \\ -2 & 2 \end{pmatrix}$$
Las varianzas son más parecidas entre sí y tienen correlación negativa.

```python
mu = np.array([0,0])
Sigma = np.matrix([[3,-2],[-2,2]])

X = np.random.multivariate_normal(mean=mu, cov=Sigma, size=500)

plt.figure(figsize=(5,5))
plt.scatter(X[:,0], X[:,1])
plt.axis("equal");
```
![[Pasted image 20250414023327.png]]
```python
plt.subplot(1,2,1)
plt.hist(X[:,0])
plt.title(r"Histograma de $X_1$")
plt.subplot(1,2,2)
plt.hist(X[:,1])
plt.title(r"Histograma de $X_2$");
```
![[Pasted image 20250414023352.png]]
```python
print(f"Media de X_1 = {np.mean(X[:,0])}")
print(f"Media de X_2 = {np.mean(X[:,1])}")
print(f"Varianza de X_1 = {np.var(X[:,0])}")
print(f"Varianza de X_2 = {np.var(X[:,1])}")

print(f"La Covarianza es:")
print(np.cov(X[:,0], X[:,1]))
print(f"La correlación es es:")
print(np.corrcoef(X[:,0], X[:,1]))
```

```
Media de X_1 = -0.021922363590680775
Media de X_2 = 0.05823511092323803
Varianza de X_1 = 2.9593028010054385
Varianza de X_2 = 1.7845437735411147
La Covarianza es:
[[ 2.96523327 -1.80925997]
 [-1.80925997  1.78812001]]
La correlación es es:
[[ 1.         -0.78572977]
 [-0.78572977  1.        ]]
```
