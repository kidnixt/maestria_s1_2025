## Ruido blanco
**Definición:** El caso más sencillo de serie temporal es un conjunto de variables aleatorias con la misma distribución y no correlacionadas. Si ${w_t}$ es una serie temporal en la cual las variables tienen media $E[w_t]=0$, varianza $Var(w_t) = \sigma_w^2 >0$ y la covarianza de $w_t$ y $w_s$ es $0$ se le denomina *ruido blanco*.  

**Definición:** Si además las variables son independientes y con la misma distribución (iid), se le denomina ruido blanco independiente.

**Definición:** En el caso que además la distribución de $w_t$ para cada $t$ sea Normal $N(0,\sigma_w^2)$ le llamamos *ruido blanco gaussiano*

```python
w = np.random.normal(size=1000,loc=0,scale=1)  # 1000 N(0,1) variates
plt.plot(w)
plt.title(r"Ruido Blanco Gaussiano $w(t)$");
```
![[Pasted image 20250416230349.png]]

```python
plt.hist(w,bins=30)
plt.title(r"Histograma de $w(t)$");
```
![[Pasted image 20250416230417.png]]

**Observación**:
Como $\mathrm{Var}(a x) = a^2 \mathrm{Var}(x)$, si tenemos ruido blanco de varianza $1$ y lo multiplicamos por $\sigma$, obtendremos ruido blanco de varianza $\sigma^2$:

```python
sigma2 = 10
w2 = np.sqrt(sigma2)*w
plt.plot(w2)
plt.title(r"Ruido Blanco Gaussiano, $\sigma^2=10$");
```
![[Pasted image 20250416230457.png]]

## Media móvil

> **Definición:** un proceso $\{x_t\}$ es una media móvil de orden $q$ o $MA(q)$ si verifica:
> $$ x_t = w_t + \theta_1w_{t-1} + ... + \theta_q w_{t-q}, $$
> donde hay $q$ lags en la media móvil, $\theta_1,...,\theta_q$ son parámetros, $\theta_q \neq 0$ y $\{w_t\}$ es ruido blanco (Gaussiano) de varianza $\sigma_w^2$.

- El valor actual depende de valores pasados del **ruido**.
- El ruido blanco puede pensarse como una media móvil de orden $0$.
- Si $q > 0$ el proceso es más suave que el ruido puro 

### Ejemplo
Considere un proceso de ruido blanco (gaussiano) $w_t$ al que se le aplica la siguiente transformación:
$$ x_t = \frac{1}{3}(w_{t-2} + w_{t-1} + w_{t})$$
Es decir, el valor de $x_t$ es un promedio de los últimos $3$ valores de un ruido blanco. ¿Qué ocurre?
```python
n=3
x = sp.signal.lfilter(1/n*np.ones(n),1,w) #Aplica el filtro de media móvil
plt.plot(w, label=r"$w(t)$")
plt.plot(x, label=r"$x(t)$");
plt.legend();
```
![[Pasted image 20250416230632.png]]

```python
plt.hist(x,bins=30);
plt.title(r"Histograma de $x(t)$");
```
![[Pasted image 20250416230702.png]]

**Observaciones:**
* El proceso $x_t$ es más *suave* que el ruido blanco, y tiene menos variabilidad.
* El promediado elimina las oscilaciones más rápidas, y comienzan a realzarse las lentas.
* Al proceso de tomar ruido blanco (o cualquier señal) y aplicarle una transformación lineal como la anterior se le llama *media móvil* o *filtrado*. De ahí el comando `filter` que se utiliza.

### Media móvil con coeficientes arbitrarios
Lo anterior se puede realizar con coeficientes cualesquiera, cambiando así la estructura del proceso.
```python
x = sp.signal.lfilter([0.1,0.2,0.3,0.4],1,w) #Aplica el filtro de media móvil
plt.plot(w, label=r"$w(t)$")
plt.plot(x, label=r"$x(t)$");
plt.legend();
```
![[Pasted image 20250416230749.png]]

## Autorregresiones

Consideremos una serie $x_t$ donde el valor actual *depende explícitamente* de los valores anteriores ponderados, más un término de ruido o *innovación* independiente que ocurre en tiempo $t$.

Más explícitamente, $x_t$ se genera por ejemplo mediante la siguiente ecuación:
$$x_t = 0.9 x_{t-1} + w_t$$
A este proceso se le denomina *autorregresivo* de orden $1$.
```python
x = sp.signal.lfilter([1],[1,-.9],w) #Aplica el filtro autorregresivo. Más adelante veremos su uso.
plt.plot(x, label=r"$x(t)$");
plt.legend();
plt.title("Autoregresivo (orden 1)");
```
![[Pasted image 20250416230910.png]]

**Observaciones:**
* El proceso $x_t$ presenta *inercia*, producto de la dependencia con los valores anteriores en el tiempo.
* Observemos que tenemos que definir la condición inicial de la serie ($x_0$, suele suponerse $0$).

### Autorregresiones en general
El procedimiento anterior permite generar procesos sumamente ricos en su estructura. 

**Definición:** un proceso autorregresivo de orden $p$ valores de la series, más una *innovación* o *ruido* que afecta solo al tiempo *t*:
$$
x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + \ldots + \phi_p x_{t-p} + w_t
$$

con $x_0,...,x_{p-1}$ especificados y $w_t$ ruido blanco (gaussiano) de varianza $\sigma_w^2$

**Parametrización alternativa:**

Otra forma de escribir el mismo proceso es
$$
a_0x_t + a1x_{t-1}+...+a_px_{t-p} = b_0w_t
$$
con $a_0=1, a_i = -\phi_i, b_0 = \sigma_w$ y en este caso $w_t$ es ruido blanco de varianza $1$. Esta es la parametrización que se utiliza en `Python`.  

#### Ejemplo:
Consideremos el proceso:
$$
x_t = x_{t-1}-0.9x_{t-2} + w_t
$$
o bien
$$
x_t - x_{t-1} + 0.9x_{t-2} = w_t
$$
con $x_0$ y $x_{-1}$ dados (por ejemplo $0$) y $\sigma_w^2 = 1$.

Este es un proceso *autorregresivo de orden 2*. En este caso $a=(1,-1,0.9)$ y $b_0=\sigma_w = 1$.

Para simular una realización en `Python` podemos:
 * Generarlo a partir de una muestra de ruido blanco como la anterior, usando la función `lfilter` de `scipy.signal` como `scipy.signal.lfilter(b, a, x)`.
 * Generarlo directamente a partir de un objeto de tipo `statsmodels.tsa.ArmaProcess`.
```python
x = sp.signal.lfilter([1],[1,-1,.9],w) #Aplica el filtro autorregresivo.
plt.plot(x, label=r"$x(t)$");
plt.legend();
plt.title(r"Autoregresivo (orden 2)");
```
![[Pasted image 20250416231000.png]]

**Observaciones:**
* En este caso, el proceso $x_t$ presenta *oscilaciones*, producto de la estructura de dependencia con los valores anteriores.

## Paseo al azar con deriva
Consideremos una señal $x_t$ que se genera a partir de ruido blanco $w_t$ mediante la siguiente ecuación:
$$ x_t = \delta + x_{t-1} + w_t.$$
Es un paseo al azar con deriva: $\delta$ es la magnitud de la deriva. Observemos que el proceso va acumulando los ruidos anteriores.
$$ 
\begin{align}
x_t &= \delta + x_{t-1} + w_t = \delta + (\delta + x_{t-2}+w_{t-1}) + w_t \\ 
&= 2\delta + w_{t-1} + w_t
\end{align}$$

y así sucesivamente.

```python
delta = 0.1  #Valor de deriva
x = np.cumsum(w+delta)  #np.cumsum() acumula los valores progresivamente.

plt.plot(x, label=r"$x(t)$");
plt.legend();
plt.title(rf"Paseo al azar con drift, $\delta$={delta}");
```
![[Pasted image 20250416231142.png]]

**Observaciones:**
El proceso no es estacionario!
 * Si la deriva es distinta de 0, esto es obvio. El valor medio de la señal va creciendo (o decreciendo) indefinidamente.
 * Si la deriva es 0, de todos modos la varianza va aumentando a medida que acumulamos más y más instancias del ruido.
 * Para ver esto último, simulemos varias realizaciones del proceso en el caso $\delta=0$.

```python 
#Paseo al azar sin deriva
w = np.random.normal(size=500,loc=0,scale=1)
x = np.cumsum(w)

plt.plot(x)
plt.title("Varias realizaciones de un paseo al azar sin deriva")

#agrego mas realizaciones
for i in range(1,20):
    w = np.random.normal(size=500,loc=0,scale=1)
    x = np.cumsum(w)
    plt.plot(x,alpha=0.3)

plt.plot(ylims=(-10,10))
```
![[Pasted image 20250416231225.png]]
### Incrementos de un paseo al azar
Si consideramos los *incrementos* del proceso, estos sí son estacionarios:
$$y_t = x_t - x_{t-1} = \delta + x_{t-1} + w_t - x_{t-1} = \delta + w_t$$
Es decir, si aplicamos la transformación $x_t \mapsto y_t = x_t - x_{t-1} = \nabla x_t$, obtenemos ruido blanco (más una media $\delta$ que marca la tendencia).

```python
w = np.random.normal(size=500,loc=0,scale=1)
delta=0.1
x = np.cumsum(w+delta)
plt.plot(x);
```
![[Pasted image 20250416231324.png]]

```python
y = np.diff(x)  #el comando diff aplica la transformación
plt.plot(y)
plt.title("Primera diferencia del paseo al azar con drift")
plt.hlines(delta,0,500, color="teal")
np.mean(y)
```

```output
np.float64(0.14571359806094164)
```
![[Pasted image 20250416231350.png]]

## Señal y ruido
Se tiene la señal:
$$ x_t = s_t + w_t$$
Siendo $s_t$ la siguiente:
$$s_t = 2\cos\left(2\pi\frac{t+15}{50}\right).$$
Aquí:
 * El $2$ multiplicando representa la *amplitud* de la señal.
 * El $1/50$ dentro del coseno es la *frecuencia*. En este caso, lleva 50 unidades de tiempo completar un ciclo.
 * El $15$ indica la *fase*, es decir, dónde se encuentran los picos. Como $\cos(2\pi n) = 1$, esta señal tendría un pico en $t=-15$ ($\cos(0)$) y luego cada $50$ unidades, por lo que el primer pico visible está en $t=35 (\cos(2\pi))$.

```python
t = np.arange(0,500)
s = 2*np.cos(2*np.pi*(t+15)/50)
plt.plot(s);
```
![[Pasted image 20250416231503.png]]
```python
w = np.random.normal(size=500,loc=0,scale=1)
x = s + w
plt.plot(x)
plt.plot(s,color="red")
plt.title(r"Señal más ruido blanco, $\sigma = 1$");
```
![[Pasted image 20250416231626.png]]
```python
sigma = 5
x = s + sigma*w
plt.plot(x)
plt.plot(s,color="red")
plt.title(rf"Señal más ruido blanco, $\sigma = {sigma}$");
```
![[Pasted image 20250416231640.png]]

**Observaciones:**
En este caso, la varianza del ruido determina cuánto podemos recuperar de la señal original. Se habla de "relación señal a ruido". Cuanto más potente el ruido, menos podemos recuperar o identificar la señal original.

## Proceso geométrico
Supongamos que tenemos una magnitud que crece *porcentualmente*. Por ejemplo, en economía, muchas magnitudes crecen un cierto porcentaje de su valor anterior:
$$s_t = (1+\theta)s_{t-1}$$
Por ejemplo, si $\theta=0.05$, crece un $5\%$ por unidad de tiempo.

```python
s=np.empty(100); #inicializo array vacío
s[0] = 1 ##inicializo el primer valor
theta = 0.05
for i in range(1,100):
    s[i] = (1+theta)*s[i-1]

plt.plot(s);
```
![[Pasted image 20250416231724.png]]

En este caso la transformación $s\mapsto \log(s)$ convierte la tendencia en lineal:
$$
\begin{align}
s'_t &= \log(s_t) = \log((1+\theta)s_{t-1}) = \log(1+\theta) + \log(s_{t-1})\\
& = \log(1+\theta) + s'_{t-1}
\end{align}$$
```python
log_s = np.log(s)
plt.plot(log_s);
```
![[Pasted image 20250416231830.png]]

Supongamos ahora que podemos asumir que existe ruido en el incremento, es decir:
$$x'_t = \log(x_t) = \log(1+\theta) + x'_{t-1} + w_t.$$
con $w_t$ ruido blanco Gaussiano. ¿Cómo se ve este proceso? ¿Cómo se ve el proceso original?
```python
w=np.random.normal(size=100,loc=0,scale=1)
log_x = np.empty(100)
log_x[0] = log_s[0]

for i in range(1,100):
    log_s[i] = np.log(1+theta) + log_s[i-1]
    log_x[i] = np.log(1+theta)  + log_x[i-1] + 0.05*w[i]

plt.plot(log_s, color="teal")
plt.plot(log_x);
```
![[Pasted image 20250416231905.png]]

El proceso en la escala original se recupera haciendo la transformación inversa al logaritmo, es decir, la exponencial.
```python
x = np.exp(log_x)

plt.plot(s, color="teal")
plt.plot(x);
```
![[Pasted image 20250416231940.png]]