# ğŸ“ˆ Clase 9 â€“ Escalabilidad de Modelos de ML

---

## ğŸ¯ Objetivo de la clase

- Entender los desafÃ­os de **servir modelos a gran escala**.
- Analizar estrategias de **escalado horizontal y vertical**.
- Aprender tÃ©cnicas para **reducir el tamaÃ±o y costo de inferencia**.
- Conocer herramientas y prÃ¡cticas para el **despliegue en dispositivos Edge**.

---

## ğŸ—ï¸ Requerimientos de infraestructura en sistemas ML reales

- Hasta **63.000 requests/segundo**
- TBs de datos por dÃ­a
- Equipos de 10sâ€“100s de data scientists
- MÃºltiples modelos en paralelo
![[Pasted image 20250605210602.png]]

---

## â˜ï¸ Nube pÃºblica vs Data centers privados

![[Pasted image 20250605210616.png]]

### Nube pÃºblica

**Pros**:
- FÃ¡cil de empezar
- Escala automÃ¡ticamente
- Pago por uso (on-demand)

**Contras**:
- Puede volverse **muy costoso** a gran escala


---

## ğŸ“Š Escalabilidad

### Horizontal (scale-out)

- Agregar mÃ¡s mÃ¡quinas (CPUs/GPUs)
- ElÃ¡stico: se ajusta segÃºn carga
	- Cuando se precisa
	- Permite reducir la infraestructura al mÃ­nimo
- Escala virtualmente ilimitada (en cloud)
	- No hay lÃ­mite de capacidad. A medida que es necesario se agregan nodos a un costo adicional


### Vertical (scale-up)

- Mejorar hardware existente
  - MÃ¡s RAM, mejor GPU/CPU
  - Almacenamiento mÃ¡s rÃ¡pido

> âš ï¸ Tiene un lÃ­mite fÃ­sico/econÃ³mico.

---

## âš™ï¸ Hardware: CPU vs GPU vs TPU

- **CPU**: General-purpose, bajo paralelismo.
- **GPU**: Alta paralelizaciÃ³n, ideal para ML.
- **TPU**: Chips especializados para TensorFlow.

![[Pasted image 20250605210812.png]]

---

## âš¡ CÃ³mo lograr inferencia mÃ¡s rÃ¡pida

1. Mejor hardware
2. OptimizaciÃ³n del modelo
3. ReducciÃ³n del tamaÃ±o del modelo

---

## ğŸ”§ TÃ©cnicas para reducir tamaÃ±o del modelo

| TÃ©cnica             | DescripciÃ³n                                         |
|---------------------|-----------------------------------------------------|
| Quantization        | Convertir pesos de float32 a int8                   |
| Pruning             | Eliminar pesos o neuronas innecesarias              |
| Knowledge distillation | Entrenar un modelo pequeÃ±o a partir de uno grande |

---

## ğŸ“‰ Quantization

![[Pasted image 20250605211153.png]]

### Â¿Por quÃ© usarla?

- Reducir tamaÃ±o del modelo
- Menor uso de memoria
- Inferencia mÃ¡s rÃ¡pida
- Menor consumo energÃ©tico

### Contras

- Puede **reducir el accuracy**
- DifÃ­cil predecir impacto
- Afecta interpretabilidad

---

### Tipos de quantization

- **Post Entrenamiento**:
	- Reduce la precisiÃ³n de la representaciÃ³n.
	- El modelo pierde un poco de accuracy
	- Considera el rendimiento del modelo y su latencia
- **Entrenamiento considerando Quantization:**
	- Inserta nodos que simulan quantization en el fordward pass.
	- Reescribe el grafo para emular la inferencia con quantization.
	- Reduce la pÃ©rdida de accuracy debido a quantization.
	- El modelo resultante ya contiene la especificaciÃ³n para su quantization.

ğŸ“š Referencia: [Quantization Aware Training â€“ TensorFlow Blog](https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html)

---

## âœ‚ï¸ Pruning

- Elimina **conexiones** o **neuronas** poco importantes.
![[Pasted image 20250605211508.png]]
### Tipos:

- **No estructurado:** elimina pesos individuales.
- **Estructurado:** elimina neuronas completas.

### Beneficios:

- Mejora velocidad en CPU y algunos aceleradores
- Reduce memoria y transferencia
- Puede **mejorar accuracy** en algunos casos
- Puede combinarse con quantization.

---

## ğŸŸï¸ Lottery Ticket Hypothesis (LTH)

> Una subred dentro de una red grande puede igualar el rendimiento de la red completa si fue bien inicializada.


*"Una red neuronal densa e inicializada al azar contiene una subred que fue inicializada de manera tal que, al entrenarse de forma aislada, puede igualar la precisiÃ³n en el conjunto de prueba de la red original despuÃ©s de entrenarse durante, como mÃ¡ximo, el mismo nÃºmero de iteraciones.â€*
 
ğŸ“„ Frankle & Carbin, ICLR 2019 â€“ ["The Lottery Ticket Hypothesis"](https://arxiv.org/pdf/1803.03635.pdf)

---

## ğŸ‘¨â€ğŸ« Knowledge Distillation

- Entrenar un **modelo simple (estudiante)** para imitar uno complejo (maestro)
- Reduce tamaÃ±o y cÃ³mputo manteniendo buena performance

> Muy Ãºtil para servir modelos en producciÃ³n con restricciones


![[Pasted image 20250605211634.png]]
---

## ğŸ“± Edge ML

Es la tendencia de adopciÃ³n de dispositivos inteligentes.

![[Pasted image 20250605211716.png]]


### Inferencia en edge vs cloud

| Aspecto              | Edge device                            | Cloud/servidor                       |
|----------------------|-----------------------------------------|--------------------------------------|
| Latencia             | Muy baja                               | Depende de conexiÃ³n                  |
| Conectividad         | No requiere                             | Necesaria                            |
| Privacidad           | Alta                                    | Riesgo de exposiciÃ³n                 |
| Recursos             | Limitados                               | Ilimitados (en cloud)                |

---

## ğŸ“‰ Limitaciones de dispositivos Edge

- Poca RAM, CPU/GPU limitada
- Consumo energÃ©tico y temperatura
- Privacidad y sin conexiÃ³n a internet (o muy poca)
- Los casos de uso generalmente requieren que sea de bajo costo, pequeÃ±os, usen poco cÃ³mputo y no se calienten.
- TamaÃ±o reducido de la app/modelo (ej: <4 GB)

---

## ğŸ§  Predicciones en dos fases

1. Modelo ligero en el dispositivo (ej: wake word: "OK Google")
2. Modelo pesado en la nube, solo si es necesario

> Ej: Google Home, Alexa â†’ el dispositivo escucha, pero solo envÃ­a la consulta si se activa

![[Pasted image 20250605211841.png]]

---

## ğŸï¸ Modelos optimizados para Edge

- Ej: **MobileNet**  
  - 40x mÃ¡s rÃ¡pido que Inception  
  - DiseÃ±ado especÃ­ficamente para visiÃ³n en celulares

ğŸ“„ Howard et al. (2017) â€“ *MobileNets: Efficient CNNs for Mobile Vision*

---

## ğŸ› ï¸ TÃ©cnicas adicionales para mejorar performance

- **Profiling** y benchmarking
- OptimizaciÃ³n de operadores
- ParalelizaciÃ³n
- Reducir precision o memoria intermedia

---

## ğŸ“¦ Despliegue en Edge

- **Core ML (Apple)**: herramienta para correr modelos en iOS
- TambiÃ©n existen:
  - TensorFlow Lite (Android)
  - MLKit 

---
