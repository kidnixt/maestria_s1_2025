# üìö Feature Engineering

Esta clase profundiza en el **Feature Engineering**, un paso crucial en el ciclo de vida de un proyecto de Machine Learning, que se enfoca en la transformaci√≥n de los datos crudos en caracter√≠sticas (features) que los modelos de ML puedan entender y utilizar eficazmente.

## üîÑ Ciclo de Vida de un Proyecto de ML: Rol de Feature Engineering

Dentro del ciclo de vida de un proyecto de ML, el **Feature Engineering** est√° intr√≠nsecamente ligado a la **Gesti√≥n de Datos** y al **Desarrollo del Modelo de ML (ML Model Development)**. Es el puente entre los datos tal como se recolectan y la forma en que el modelo los consume para aprender y hacer predicciones.

## üïµÔ∏è‚Äç‚ôÄÔ∏è Feature Engineering: Manejo de Valores Faltantes

Los valores faltantes en un dataset son un desaf√≠o com√∫n y su manejo es una parte vital del Feature Engineering. Es crucial entender la naturaleza de estos datos faltantes:

* **No aleatoriamente (Missing Not At Random - MNAR):** La raz√≥n por la que un valor falta est√° directamente relacionada con el valor en s√≠ mismo.
    * *Ejemplo:* Personas con salarios muy altos o muy bajos pueden ser menos propensas a revelarlos en una encuesta.
* **Aleatoriamente por segmento (Missing At Random - MAR):** Los valores faltantes afectan solo a un segmento de los datos.
    * *Ejemplo:* La edad de una persona puede faltar m√°s frecuentemente en una encuesta si la persona pertenece a un g√©nero espec√≠fico.
* **Totalmente aleatorio (Missing Completely At Random - MCAR):** No hay un patr√≥n discernible detr√°s de los datos faltantes; son eliminados de forma puramente aleatoria. Esto es lo menos com√∫n, ya que generalmente hay una raz√≥n o mecanismo que explica la ausencia de datos.

### Estrategias para Valores Faltantes

Existen dos estrategias principales para manejar valores faltantes:

1.  **Eliminaci√≥n:** Consiste en remover los datos que contienen valores faltantes.
    * **Por columna:** Eliminar una columna si tiene muchos valores faltantes.
        * *Riesgo:* Puede hacer que el modelo pierda se√±al y sea menos preciso.
    * **Por fila:** Eliminar filas completas con valores faltantes.
        * *Ventaja:* Puede funcionar bien si los datos faltantes son totalmente aleatorios o muy pocos, ya que no deber√≠an afectar significativamente.
        * *Riesgo:* Si los datos faltantes son aleatorios por segmento, puede eliminar datos importantes o generar sesgos.

2.  **Imputaci√≥n:** Rellenar los valores faltantes con un valor definido.
    * **Valores por defecto:** Como un string vac√≠o para datos categ√≥ricos.
    * **Estad√≠sticas descriptivas:** Promedio, mediana o moda de la columna correspondiente.
    * *Riesgos de la imputaci√≥n:*
        * **Introducir sesgo:** Si el valor imputado no refleja la realidad.
        * **Agregar ruido:** Si el valor imputado no es preciso.
        * **Data leakage:** Si la imputaci√≥n se hace usando estad√≠sticas del conjunto de prueba (ver m√°s adelante).

## üõë ¬°Cuidado con los Outliers!

Es importante distinguir entre deshacerse de **entradas inv√°lidas** y deshacerse de **datos v√°lidos** que son *outliers*. Eliminar outliers v√°lidos o aplicarles *clipping* (recortar valores extremos) puede hacer que un modelo trate casos diferentes de igual manera, perdiendo la capacidad de generalizar para esas situaciones.

* *Ejemplo:* Aplicar *clipping* a la edad de una madre puede hacer que un modelo trate igual a una madre de 45 a√±os que a una de 50, cuando biol√≥gicamente son casos muy distintos.

## üõ†Ô∏è ¬øQu√© hace el Feature Engineering?

El Feature Engineering es el proceso de transformar datos crudos para crear **vectores de features** que un modelo de ML pueda utilizar. Esto implica:
* Mapear datos crudos a formatos num√©ricos o vectoriales.
* Convertir enteros a puntos flotantes o viceversa.
* Normalizar valores num√©ricos.
* Transformar strings y datos categ√≥ricos en vectores.
* Mapear datos de un espacio a otro.

### Ejemplos de Transformaciones Comunes

| Instancia        | Todo el dataset |
| :--------------- | :-------------- |
| Clipping         | Scaling         |
| Hashing encoding | OHE             |
| Binning          | Bucketizing     |
| Binary encoding  | Label encoding  |
| Quantization     | TF-IDF          |

## üß† Embeddings: Representaci√≥n de Datos Aprendible

Los **Embeddings** son representaciones aprendibles de datos que mapean informaci√≥n de **alta cardinalidad** (como im√°genes o texto) a un **espacio de menores dimensiones**.

* **Prop√≥sito:** Preservar la informaci√≥n relevante para la tarea de aprendizaje.
* **Beneficios:** Permiten usar datos complejos para tareas como clustering o clasificaci√≥n.
* **Problema principal:** Pueden conllevar una p√©rdida de la representaci√≥n original de los datos.
* **Ganancia:** Proporcionan informaci√≥n sobre la cercan√≠a entre √≠tems y su contexto (ej. relaciones sem√°nticas entre palabras).
    * *Ejemplo:* En el espacio de embeddings de palabras, la distancia vectorial entre "hombre" y "mujer" puede ser similar a la distancia entre "rey" y "reina", o entre verbos en diferentes tiempos ("caminando" y "camin√≥").

![[Pasted image 20250602184225.png]]

Los **Autoencoders** son un tipo de red neuronal utilizada para aprender representaciones (embeddings) de datos de forma no supervisada, comprimiendo la entrada en un "cuello de botella" (bottleneck) y luego reconstruy√©ndola.

![[Pasted image 20250602184240.png]]

## üß∫ Data Leakage (Filtraci√≥n de Datos)

El **Data Leakage** ocurre cuando se utiliza informaci√≥n del conjunto de prueba (o validaci√≥n) para entrenar el modelo, lo que resulta en una sobreestimaci√≥n del rendimiento del modelo en datos nuevos. Es crucial entender la diferencia entre sesgos y data leakage.

### Causas Comunes de Data Leakage 

* **Splitting de datos correlacionados por tiempo:** Si los datos tienen una dependencia temporal (ej. series de tiempo), un splitting aleatorio puede exponer el modelo a informaci√≥n "futura" en el conjunto de entrenamiento.
* **Preprocesamiento global antes de splitting:** Realizar operaciones como *scaling* o normalizaci√≥n en todo el dataset antes de dividirlo en conjuntos de entrenamiento, validaci√≥n y prueba, hace que las estad√≠sticas de transformaci√≥n (ej. media, desviaci√≥n est√°ndar) incluyan informaci√≥n del conjunto de prueba.
* **Imputaci√≥n de datos con estad√≠sticas de test:** Usar la media o mediana del conjunto de test para imputar valores faltantes en el conjunto de entrenamiento.
* **Mal manejo de datos duplicados previo a splitting:** Si hay datos duplicados y estos se reparten entre training y test, el modelo "ve" datos del test durante el entrenamiento.
* **Filtraci√≥n de datos de grupos:** Si hay grupos o identificadores √∫nicos en los datos, es importante asegurar que las divisiones se hagan a nivel de grupo para evitar que informaci√≥n de un grupo se filtre a trav√©s de los splits.
* **Features correlacionadas con el target:** Algunas features pueden estar demasiado correlacionadas con la variable objetivo de una manera que no se generalizar√° a datos nuevos.

### Atacando la Filtraci√≥n (Mitigaci√≥n)

* **Medir la correlaci√≥n entre features y el target:** Investigar si alguna feature muestra una correlaci√≥n inusualmente alta, ya que podr√≠a indicar un leakage.
* **Estudios de ablaci√≥n:** Quitar una feature del modelo y observar el impacto en el rendimiento. Si una feature es excesivamente determinante, investiga si hay leakage.
* **√çdem para nuevas features:** Aplicar el mismo escrutinio a cualquier nueva feature que se incorpore.
* **Usar el test split √∫nicamente para evaluaci√≥n final:** La regla de oro es que el conjunto de test solo se utilice una √∫nica vez al final del proceso para una evaluaci√≥n imparcial del modelo final.

## ‚ö†Ô∏è "Ojo": Advertencias sobre el N√∫mero de Features. 

Cuantas m√°s features se utilizan en un modelo, mayores son los riesgos y complejidades:
* M√°s chances de **filtraci√≥n de datos (data leakage)**.
* Puede causar **overfitting** (el modelo aprende el ruido de los datos de entrenamiento en lugar del patr√≥n real).
* Mayor necesidad de **memoria y c√≥mputo**.
* Mayor **latencia** en la inferencia.
* Aumento de la **dependencia de datos** y la **deuda t√©cnica**.

Es fundamental considerar la **importancia de la feature y su capacidad de generalizaci√≥n**.

## üßê Selecci√≥n de Features (Feature Selection)

La selecci√≥n de features es el proceso de identificar el subconjunto m√°s relevante de features en los datos de entrenamiento para construir un modelo.

![[Pasted image 20250602184355.png]]

### ¬øPor qu√© es Necesaria la Selecci√≥n de Features?

* **Reducir almacenamiento:** Menos datos para guardar.
* **Reducir input/output:** Menos datos que leer y escribir.
* **Minimizar costos de entrenamiento e inferencia:** Menos c√°lculos.
* **Reducir dependencias de datos:** Menos complejidad en el pipeline de datos.

### Tipos de Selecci√≥n de Features

1.  **Supervisada:** Utiliza la variable objetivo (target) para seleccionar las features que m√°s contribuyen al rendimiento del modelo.
2.  **No supervisada:** No utiliza el target; se enfoca en remover features correlacionadas o redundantes.

#### M√©todos de Selecci√≥n de Features Supervisada

* **M√©todos de Filtro:** Eval√∫an cada feature individualmente sin entrenar un modelo. Son r√°pidos y se usan como primera pasada.
    * **Correlaci√≥n entre features:** Detecta redundancia calculando la correlaci√≥n entre variables independientes. Si dos features est√°n altamente correlacionadas (ej. > 0.9), se puede conservar solo una.
    * **Correlaci√≥n con el target:** Mide c√≥mo se relaciona cada feature con la variable objetivo.
        * *Ejemplo:* Correlaci√≥n de Pearson para regresi√≥n, o Chi-cuadrado/Mutual Information para clasificaci√≥n.
    * **Selecci√≥n de features univariada:** Aplica un test estad√≠stico (ej. ANOVA, Chi-cuadrado) a cada variable y selecciona las m√°s significativas.
* **M√©todos Wrapper:** Entrenan m√∫ltiples modelos con diferentes combinaciones de features para elegir la mejor. Son m√°s precisos pero m√°s lentos.
    * **Forward Selection:** Empieza sin features y agrega una por una la que m√°s mejora el rendimiento hasta que no haya m√°s mejoras.
    * **Backward Elimination:** Empieza con todas las features y quita una por una la que menos impacta el rendimiento, hasta que quitar una empeora el modelo.
    * **Selecci√≥n Recursiva de Features (RFE):** Usa un modelo (ej. regresi√≥n, √°rbol) para estimar la importancia de las features y elimina recursivamente la menos importante.
* **M√©todos Embebidos:** La selecci√≥n de features ocurre durante el entrenamiento del modelo, como parte de su mecanismo interno.
    * **Regularizaci√≥n L1 (Lasso):** Penaliza los coeficientes del modelo lineal, haciendo que algunos se vuelvan cero, lo que equivale a eliminar autom√°ticamente esas features.
    * **Importancia de las features:** Algunos modelos basados en √°rboles (Random Forest, XGBoost) calculan autom√°ticamente la importancia de cada feature (ej. cu√°ntas veces se usa para dividir los datos). Este ranking puede usarse para seleccionar las m√°s relevantes.

## üìä Feature Importance y SHAP

### Generalizaci√≥n de la Importancia de Features 

* **Extensi√≥n (Global vs Local):** La importancia puede ser general para todo el modelo o espec√≠fica para una predicci√≥n individual.
* **Distribuci√≥n:** C√≥mo se distribuye la importancia entre las features.
* **Conocimiento de dominio:** Siempre es crucial revisar las variables junto con expertos del dominio.

### Herramientas para Entender la Importancia.

* La importancia en **√°rboles de decisi√≥n** (y ensambles como Random Forest) es r√°pida y simple de obtener.
* **SHAP (SHapley Additive exPlanations):** Una t√©cnica que mide la contribuci√≥n de cada feature a una predicci√≥n individual (`f(x)`) y tambi√©n la contribuci√≥n general de las features al modelo. Permite entender por qu√© un modelo hizo una predicci√≥n espec√≠fica y c√≥mo influyen las features en el comportamiento general del modelo.

## üìù Resumen Clave: Consideraciones en Feature Engineering

* Generalmente requiere **conocimiento de dominio** y el input de expertos que no son ingenieros.
* Dividir los **datos temporales correctamente por tiempo** para evitar leakage.
* Aplicar **transformaciones basadas en estad√≠sticas globales despu√©s de realizar el splitting** de los datos (ej. scaling, normalizaci√≥n).
* Asegurar el **linaje de datos y la trazabilidad** para saber de d√≥nde provienen las features y c√≥mo se transformaron.
* Entender la **importancia de las features y su capacidad de generalizaci√≥n**.
* **Quitar features que no son √∫tiles** mejora el uso de recursos y previene problemas como overfitting o data leakage.
* El Feature Engineering es una **fuente com√∫n de bugs dif√≠ciles de detectar** en los sistemas de ML.

## ‚öôÔ∏è Preprocesamiento de Datos

El preprocesamiento es el paso de **preparar los datos** para que sean √∫tiles para entrenar modelos y para que los modelos puedan aprender de ellos eficazmente.

### Preprocesando Datos a Gran Escala

Para manejar terabytes de datos, se requieren:
* **Frameworks de procesamiento para grandes vol√∫menes de datos** (ej. Apache Spark, Google Dataflow, Apache Beam).
* **Reproducibilidad:** Es crucial asegurar que las transformaciones de datos puedan ser replicadas consistentemente. Esto implica manejar la aleatoriedad (semillas aleatorias, splitting repetible), el linaje de datos y evitar el *training-serving skew*.
* **Transformaciones consistentes entre training y serving:** Las mismas transformaciones deben aplicarse tanto en el entrenamiento como en la inferencia para evitar desviaciones (skew).

### Pipelines de ETL para Procesamiento de Datos

* **Batch (por lotes):**
    * **Extract (Extraer):** Datos de fuentes como CSVs, XML, JSON, APIs.
    * **Transform (Transformar):** Procesamiento de los datos.
    * **Load (Cargar):** Los datos transformados se cargan en un Data Lake o Data Warehouse (ej. BigQuery).
    * Resulta en un job de predicci√≥n/entrenamiento batch con los datos preparados.

![[Pasted image 20250602184747.png]]


* **Streaming (flujo continuo):**
    * **Extract (Extraer):** Datos de streaming en tiempo real (ej. Apache Kafka, Google Cloud Pub/Sub).
    * **Transform (Transformar):** Procesamiento en tiempo real.
    * **Load (Cargar):** Los datos transformados pueden ir a otra pipeline de streaming, Data Warehouse o Data Lake.
    * Consumido por pipelines de predicci√≥n/entrenamiento en tiempo real.

![[Pasted image 20250602184757.png]]

## üöß Training-Serving Skew (Sesgo de Entrenamiento-Servicio)

El *training-serving skew* ocurre cuando hay una **discrepancia en el rendimiento entre el modelo entrenado y el modelo en producci√≥n**, debido a diferencias en c√≥mo se manejan los datos en las fases de entrenamiento e inferencia. Para evitarlo, las transformaciones sobre el input **deben ser las mismas** en ambas fases.

### Estrategias para Evitar el Skew 

* **Reuso de funciones:** Utilizar las mismas funciones de preprocesamiento tanto en entrenamiento como en inferencia.
    * *Ejemplo:* Preprocesar con Apache Beam y guardar las transformaciones de `tf.transform` como *artifacts* que luego pueden ser usadas en serving
* **Incluir el preprocesamiento dentro del modelo:**
    * En Keras, usar capas de preprocesamiento personalizadas (`custom preprocessing layers`).
    * En Scikit-learn, integrar el preprocesamiento en pipelines y transformadores.

## üó∫Ô∏è ¬øD√≥nde y Cu√°ndo Realizar el Preprocesamiento?

La decisi√≥n de d√≥nde realizar el preprocesamiento (como parte de la pipeline de datos o dentro del modelo) implica un *trade-off* entre **eficiencia, experimentaci√≥n, mantenibilidad y flexibilidad** 

| Etapa                                          | Pros                                                                              | Contras                                                                          |
| :--------------------------------------------- | :-------------------------------------------------------------------------------- | :------------------------------------------------------------------------------- |
| **Preprocesamiento (en la pipeline de datos)** | Corre una √∫nica vez, considera todo el dataset.                                   | Requiere reproducir las transformaciones en serving, iteraci√≥n lenta.            |
| **Con el modelo (en capas del modelo)**        | Iteraci√≥n f√°cil, garant√≠as sobre las transformaciones (se aplican siempre igual). | Transformaciones m√°s costosas, puede aumentar la latencia, riesgo de batch skew. |

**TF Transform** es una herramienta que facilita la consistencia de las transformaciones entre entrenamiento y serving, permitiendo aplicar las mismas l√≥gicas definidas en el pipeline de Beam directamente en el grafo de TensorFlow para inferencia.

![[Pasted image 20250602185001.png]]

## üóÑÔ∏è Feature Store

Una **Feature Store** es una plataforma centralizada que facilita a los equipos de ML el **descubrimiento, uso y compartici√≥n** de features.

* **Beneficios Clave:**
    * Evita la **duplicaci√≥n** de esfuerzos en Feature Engineering.
    * Permite **monitorear la calidad de los datos** de las features.
    * Gestiona **permisos de acceso**.
    * Facilita el cumplimiento de **regulaciones** (ej. GDPR).
    * **Precomputa y almacena features** en modo batch y en tiempo real.
    * Es **escalable y performante** tanto para entrenamiento *offline* como para inferencia *online*.

Una Feature Store act√∫a como un puente entre el Feature Engineering y el Desarrollo del Modelo, almacenando y sirviendo las features de manera consistente y eficiente.