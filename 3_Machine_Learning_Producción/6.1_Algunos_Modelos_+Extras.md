
## 🚀 Modelos Basados en Árboles

Los modelos basados en árboles, como los *Gradient Boosted Trees* (ej. XGBoost, LightGBM), son muy populares por su rendimiento y eficiencia.

* **Pros:**
    * Excelente rendimiento en datos tabulares.
    * Relativamente rápidos de entrenar.
    * No requieren escalar o normalizar los datos.
    * Robustos a *outliers* y a valores faltantes.
    * Permiten conocer la importancia de las *features*.
* **Contras:**
    * Menos adecuados para datos no estructurados como imágenes o texto.
    * Puede ser difícil interpretarlos a nivel global si son muy profundos.
    * Tienden a *overfit* fácilmente si no se configuran correctamente.

## 🧠 Redes Neuronales

Las Redes Neuronales (NN) han revolucionado muchas áreas del ML, especialmente con los avances en Deep Learning.

* **Pros:**
    * Excelente rendimiento en datos no estructurados (imágenes, texto, audio).
    * Capaces de aprender características complejas y representaciones de datos (embeddings).
    * Potencial para superar el rendimiento humano en ciertas tareas.
* **Contras:**
    * Requieren grandes cantidades de datos.
    * Tiempo de entrenamiento y recursos computacionales (GPUs) elevados.
    * Difícil de interpretar ("cajas negras").
    * Son propensas a *overfitting*.
    * Sensibles a la escala de los datos.

## 📈 Modelos Lineales

Los modelos lineales (ej. Regresión Lineal, Regresión Logística) son una opción básica pero muy potente.

* **Pros:**
    * Rápidos de entrenar y predecir.
    * Fáciles de interpretar.
    * Menos propensos a *overfitting* si el número de features es limitado.
    * Ideales como *baselines*.
* **Contras:**
    * Rendimiento inferior en problemas con relaciones no lineales.
    * Necesitan *feature engineering* extensivo para capturar no linealidades.
    * Asumen relaciones lineales entre las variables y el *target*.

## 🌲 Modelos Ensemble (Bagging vs. Boosting)

Los modelos *ensemble* combinan múltiples modelos (típicamente árboles de decisión) para mejorar el rendimiento y la robustez.

### Bagging (ej. Random Forest)
* **Concepto:** Entrena múltiples modelos independientes en subconjuntos aleatorios de datos (con reemplazo) y promedia sus predicciones.
* **Ventaja:** Reduce la varianza y el *overfitting*. Si los errores de los modelos individuales no están correlacionados, el *ensemble* puede ser muy efectivo.
* **Ejemplo:** Random Forest entrena muchos árboles de decisión, cada uno con una parte aleatoria de los datos y las *features*, y luego promedia sus resultados.

### Boosting (ej. Gradient Boosting, XGBoost)
* **Concepto:** Entrena modelos secuencialmente, donde cada nuevo modelo corrige los errores del anterior. Se enfoca en los ejemplos que los modelos previos clasificaron mal.
* **Ventaja:** Reduce el sesgo y puede lograr un rendimiento muy alto. Es mejor cuando hay un sesgo de predicciones alto.
* **Ejemplo:** Un modelo inicial hace predicciones, y los errores de estas predicciones son utilizados para entrenar el siguiente modelo, y así sucesivamente.

| Característica              | Bagging                                 | Boosting                                     |
| :-------------------------- | :-------------------------------------- | :------------------------------------------- |
| **Paralelización**          | Sí, los modelos se entrenan en paralelo | No, los modelos se entrenan secuencialmente  |
| **Sesgo**                   | Tiende a ser más alto                   | Tiende a ser más bajo                        |
| **Varianza**                | Tiende a ser más bajo                   | Tiende a ser más alto                        |
| **Overfitting**             | Menos propenso                          | Más propenso si no se controla bien          |
| **Complejidad**             | Más simple                              | Más complejo                                 |
| **Interpretabilidad**       | Media                                   | Baja                                         |
| **Tiempo de entrenamiento** | Rápido                                  | Lento                                        |
| **Resistencia a outliers**  | Alta                                    | Media                                        |
| **Ejemplos**                | Random Forest, Bagging Trees            | Gradient Boosting, XGBoost, LightGBM         |

## 📏 Modelos de Clasificación: Probabilidad vs. Clase Directa

Cuando se hace clasificación, es importante saber si el modelo debe retornar la **probabilidad** de pertenecer a una clase o directamente la **clase predicha**. Esto impacta la **calibración** del modelo

### Calibración

Un modelo está **calibrado** si sus probabilidades predichas corresponden a las probabilidades reales del evento
* *Ejemplo:* Si un modelo predice un 80% de probabilidad de lluvia, debería llover el 80% de las veces que da esa predicción.
* **Importancia:** Crucial para la toma de decisiones basada en riesgo (ej., en medicina o finanzas), donde la confianza en la predicción es vital.

## 🚦 Sesgos en la Predicción

Es importante considerar los sesgos en la predicción:
* **Sesgo de cobertura:** Cuando los datos de entrenamiento no representan adecuadamente a la población de inferencia.
* **Sesgo de muestreo:** Si el método de recolección de datos favorece ciertas partes de la población.
* **Sesgo de etiqueta:** Errores o inconsistencias en el proceso de etiquetado.
* **Sesgo de features:** Si las *features* no son adecuadas o están mal construidas.
* **Sesgo de optimización:** Si el modelo optimiza una métrica que no se alinea con el objetivo de negocio.

## 🪞 Interpretabilidad del Modelo

La interpretabilidad es la capacidad de entender cómo o por qué un modelo tomó una decisión específica.

### Razones para la Interpretabilidad

* **Confianza:** Generar confianza en los usuarios y desarrolladores.
* **Identificar sesgos:** Detectar si el modelo está discriminando.
* **Debugging:** Entender por qué el modelo falla.
* **Cumplimiento regulatorio:** En industrias como finanzas o salud, es a menudo un requisito legal.
* **Mejora del modelo:** Identificar áreas donde el modelo puede mejorar.
* **Descubrimiento científico:** Obtener insights de los datos.

### Tipos de Modelos (de más interpretable a menos)

* **Lineal/Regresión Logística:** Muy interpretables, se puede ver la contribución de cada *feature*.
* **Árboles de decisión:** Razonablemente interpretables; se pueden visualizar las reglas.
* **XGBoost/Random Forest:** Menos interpretables que un solo árbol, pero se puede obtener la importancia de las *features*.
* **Redes neuronales:** Generalmente las menos interpretables ("cajas negras").



## 🧱 Baselines y Modelos Simples

Es fundamental comenzar siempre con una **baseline** simple y entender su rendimiento:
* **Baseline:** Un modelo simple o una heurística que sirve como punto de referencia para evaluar modelos más complejos.
    * *Ejemplo:* Predecir siempre la clase más frecuente en un problema de clasificación.
* **Importancia:** Un modelo simple a veces puede ser lo suficientemente bueno y evitar complejidades innecesarias. Siempre se debe evaluar si las ganancias de un modelo más complejo justifican los costos adicionales (computacionales, de mantenimiento, etc.).
