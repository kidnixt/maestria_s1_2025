# âš™ï¸ Clase 7 â€“ Entrenamiento y EvaluaciÃ³n Offline

---

## ğŸ¯ Objetivo de la clase

- Comprender cÃ³mo entrenar modelos de forma eficiente en la prÃ¡ctica.
- Aprovechar al mÃ¡ximo los recursos de cÃ³mputo disponibles.
- Conocer tÃ©cnicas de entrenamiento distribuido y evaluaciÃ³n offline.
- Aprender a trazar y versionar experimentos de ML.

---

## ğŸ§  Entrenamiento eficiente

### Problemas actuales

- El entrenamiento consume **cada vez mÃ¡s recursos**:
  - MÃ¡s datos
  - Modelos mÃ¡s grandes
- MÃ¡s tiempo implica mÃ¡s `epochs`, pero tambiÃ©n mÃ¡s ineficiencia.

### Soluciones

- **Optimizar la ingesta de datos**:  
  - Usar formatos como `TFRecords`
  - Preprocesar embeddings si es Transfer Learning
  - Procesamiento paralelo durante el entrenamiento

- **Aprovechar la GPU al mÃ¡ximo**:  
  - Usar herramientas de monitoreo (`nvidia-smi`, AMD Monitor)
  - Entrenar con mÃºltiples GPUs o nodos

![[Pasted image 20250607132825.png]]

---

## ğŸ§± Entrenamiento distribuido

### Objetivo:
Mantener GPUs y procesadores **ocupados todo el tiempo**.

### Tipos de paralelismo:

| Tipo                  | DescripciÃ³n                                                  |
| --------------------- | ------------------------------------------------------------ |
| Paralelismo de datos  | Cada mÃ¡quina entrena el mismo modelo con diferentes datos.   |
| Paralelismo de modelo | Se divide el modelo en partes, cada nodo entrena una parte.  |

> ğŸ’¡ Paralelismo de modelo permite usar modelos mÃ¡s grandes, pero es mÃ¡s complejo.


![[Pasted image 20250607132937.png]]
---

## âš™ï¸ Estrategias de paralelismo con TensorFlow

| Estrategia                  | DescripciÃ³n                                          |
|-----------------------------|------------------------------------------------------|
| `MirroredStrategy`          | Copia el modelo en mÃºltiples GPUs locales           |
| `MultiWorkerMirroredStrategy` | Igual, pero entre varias mÃ¡quinas                   |
| `TPUStrategy`               | Para entrenar en TPUs                                |
| `OneDeviceStrategy`         | Ãštil para debugging (una sola GPU o CPU)            |

---

## ğŸ” Ciclo en paralelismo de datos

1. Cada GPU toma un fragmento del batch.
2. Cada GPU hace `forward` en paralelo.
3. Cada GPU hace `backpropagation`.
4. Se hace **all-reduce** para sincronizar gradientes.
5. Se actualizan los parÃ¡metros del modelo en conjunto.

> Todo el modelo se mantiene sincronizado entre GPUs.

---

## âš ï¸ DesafÃ­os del paralelismo

- **Scaling del learning rate**: requiere ajustes si el batch size cambia.
- **SincronizaciÃ³n**:
  - SincrÃ³nico: mÃ¡s preciso, pero puede haber â€œstragglersâ€.
  - AsincrÃ³nico: mÃ¡s rÃ¡pido, pero puede generar inconsistencia.

---

## ğŸ§© Paralelismo de modelo y pipeline

- Se divide el **modelo** entre varias mÃ¡quinas.
- Cada mÃ¡quina procesa una parte del modelo.
- Se puede combinar con paralelismo de datos.

### GPipe â€“ Paralelismo por pipeline

- Divide el minibatch en **microbatches**.
- Cada nodo procesa un microbatch en una parte del modelo.
- Permite entrenar **redes gigantes** que no caben en una sola GPU.

---

## ğŸ§  TÃ©cnicas adicionales

- **Gradient accumulation**: divide un batch en mini-batches, suma gradientes y actualiza una vez.
- **Memory swapping**: mueve activaciones de GPU a RAM externa cuando no se usan.

---

## ğŸŒ Federated Learning

Entrenamiento colaborativo sin centralizar los datos.

- Cada cliente (ej. celular) entrena con sus datos locales.
- Solo se comparten los gradientes.
- Evita problemas de privacidad y reduce trÃ¡fico de datos.

![[Pasted image 20250607133100.png]]

---

## ğŸ“ EvaluaciÃ³n offline

### PropÃ³sito

- Medir el rendimiento de un modelo **antes de ponerlo en producciÃ³n**.
- Evaluar **robustez, fairness y generalizaciÃ³n**.

---

## ğŸ§ª Baselines

| Tipo                    | Ejemplo                          |
|-------------------------|----------------------------------|
| Aleatorio               | Output aleatorio                 |
| Regla heurÃ­stica        | Predecir clase mÃ¡s comÃºn         |
| Modelos simples         | Logistic Regression              |
| Humano                  | Performance humano (si aplica)   |
| Sistema anterior        | Comparar contra soluciÃ³n previa  |

> âš ï¸ â€œBuenoâ€ no siempre significa â€œÃºtilâ€.

---

## ğŸ§¬ MÃ©todos de evaluaciÃ³n

### 1. Perturbaciones

- Agregar ruido o modificar inputs.
- Ej: ruido en audio, clipping de texto.
- Objetivo: ver si el modelo **resiste cambios leves**. Queremos ver si es robusto.

### 2. Invariantes

- Cambiar features irrelevantes: el output **debe mantenerse**. No debe ser propenso a sesgos o ser injusto con ciertos segmentos.
- Ej: cambiar nombre/gÃ©nero en un CV no deberÃ­a afectar el screening o la decisiÃ³n tomada por un modelo.

### 3. Expectativa direccional

- Cambios en el input deben provocar cambios lÃ³gicos en el output.
- Ej: mÃ¡s superficie â†’ precio mÃ¡s alto (en modelos de casas)

---

## ğŸ§® EvaluaciÃ³n por segmentos

Evaluar desempeÃ±o **por subgrupo** (gÃ©nero, edad, clase, etc.)

| Modelo   | Clase mayoritaria | Clase minoritaria | Global |
|----------|-------------------|-------------------|--------|
| A        | 98%               | 80%               | 96.2%  |
| B        | 95%               | 95%               | 95.0%  |

> A puede parecer mejor globalmente, pero tiene un sesgo claro.

---

## ğŸ§  Paradoja de Simpson

Un modelo puede ser **peor en cada subgrupo**, pero mejor en el total.  
Ejemplo clÃ¡sico: admisiones en UC Berkeley.

> Evaluar solo a nivel global puede ocultar **discriminaciÃ³n estructural**.

---

## ğŸ§© Identificando segmentos crÃ­ticos

- Conocimiento del dominio
- AnÃ¡lisis manual de errores
- Herramientas como:
  - `Slice Finder`
  - Algoritmos de detecciÃ³n de subgrupos

---

## ğŸ“ CalibraciÃ³n de modelos

Un modelo calibrado:  
> Si predice un 70% de probabilidad, acierta el 70% de las veces.

### Aplicaciones:
- Publicidad (CTR): predecir probabilidad de clic
- Recomendaciones: ranking de Ã­tems

### Herramientas:
- `sklearn.calibration.CalibratedClassifierCV`
- Temperature scaling

---

## ğŸ” Debug y trazabilidad en ML

### DesafÃ­os:

- Los errores pueden no ser obvios (fallos silenciosos).
- Arreglos requieren reentrenamiento.
- Muchos componentes interdependientes.

---

## ğŸ” Â¿QuÃ© trazar?

- Curvas de pÃ©rdida (loss)
- MÃ©tricas (accuracy, F1)
- Logs por muestra (input, predicciÃ³n, etiqueta)
- Tiempo de entrenamiento
- Recursos usados
- HiperparÃ¡metros y configuraciÃ³n

---

## ğŸ§° Herramientas para trazabilidad

- Open source: `TensorBoard`
- Comerciales: `Weights & Biases`, `Comet.ml`

---

## ğŸ““ Notebooks y producciÃ³n

- BuenÃ­simos para experimentar, pero:
  - No suelen ir a producciÃ³n.
  - Se recomienda convertirlos:
    - `nbconvert`: `.ipynb` â†’ `.py`
    - `jupytext`: control de versiones
    - `nbdime`: comparaciones entre versiones

---

## ğŸ§¬ Versionado en ML

### Â¿QuÃ© versionar?

- **CÃ³digo**: `Git`
- **Modelos entrenados**
- **Datos**
- **ParÃ¡metros e hiperparÃ¡metros**

### Herramientas

| Elemento      | Herramienta                     |
|---------------|---------------------------------|
| Datos         | `DVC`, `Git LFS`, `Pachyderm`   |
| Modelos       | `MLflow`, `Azure ML`, `Comet`   |

---

## ğŸ§± Registro y linaje de modelos

- **Linaje:** Â¿CÃ³mo llegamos a este modelo?
  - Dataset, cÃ³digo, parÃ¡metros, entorno
- **Model Registry:**
  - Repositorio central de modelos
  - Guarda artefactos, versiones, metadatos

