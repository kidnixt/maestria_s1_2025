# ğŸš€ Clase 8 â€“ Serving de Modelos


---

## ğŸ¯ Objetivo de la clase

- Entender cÃ³mo **llevar modelos a producciÃ³n** y exponer predicciones.
- Comparar estrategias de despliegue: batch, online, streaming.
- Explorar trade-offs entre latencia, throughput y costo.
- Conocer herramientas para empaquetar y servir modelos (Docker, servicios gestionados, etc).

---

## ğŸ”§ Etapas para poner un modelo en producciÃ³n

1. Entrenamiento del modelo.
2. ElecciÃ³n del entorno de ejecuciÃ³n.
3. Empaquetado (container, web service, etc).
4. **Despliegue (serving)**.
5. ExposiciÃ³n de predicciones a otros sistemas.
6. Monitoreo y mantenimiento.

---

## ğŸ§­ Opciones de despliegue

- **On-premise:** infraestructura propia.
- **Cloud:** servicios como AWS, GCP, Azure.
- **Edge:** dispositivos fÃ­sicos (celulares, sensores).

> Ej: Un modelo de detecciÃ³n de defectos puede correr en una fÃ¡brica (*edge*) sin depender de conexiÃ³n a la nube.

---

## ğŸŒ Web app vs API

- Web app: muestra resultados al usuario final.
- **API/servidor de modelo:** expone un endpoint que recibe inputs y devuelve predicciones.

> Ej: `/predict` â†’ recibe JSON, responde JSON con resultados.

---

## ğŸ§± Sistemas de despliegue

Sistemas que:

- Centralizan el despliegue de modelos.
- Exponen endpoints para hacer predicciones.
- No requieren crear aplicaciones especÃ­ficas.
- Permiten actualizar o revertir modelos fÃ¡cilmente.
- Automatizan escalabilidad y versionado.

> ğŸ“¦ Ejemplos: TensorFlow Serving, TorchServe, SageMaker endpoints

---

## ğŸ—ï¸ Arquitecturas de despliegue

### VM tradicional

- Sistema operativo completo, configuraciÃ³n manual.
- MÃ¡s pesado y menos portÃ¡til.

### Contenedores (Docker)

- Livianos y replicables.
- Permiten definir exactamente:
  - SO base
  - Dependencias (paquetes)
  - Variables de entorno
  - Modelo y cÃ³digo de inferencia
  - Comandos a ejecutar

> ğŸ“Œ Ideal para despliegue cloud o CI/CD

![[Pasted image 20250605212540.png]]

---

## ğŸ³ Docker: empaquetar la soluciÃ³n

- Define en un `Dockerfile` los pasos para construir la imagen.
- Facilita compartir modelos en cualquier entorno.
- Compatible con servicios cloud, edge o locales.

> Ej: montar un servicio Flask + modelo en Docker y subirlo a AWS.

---

## â˜ï¸ Servicios gestionados (MaaS)

Plataformas cloud nativas:

- Ej: **Vertex AI (GCP)**, **SageMaker (AWS)**

CaracterÃ­sticas:

- Predicciones **de baja latencia**
- Batching dinÃ¡mico
- Escalado automÃ¡tico
- Uso de GPU/TPU si es necesario
- Alta disponibilidad

---

## ğŸ“ˆ MÃ©tricas clave en serving

| MÃ©trica     | DescripciÃ³n                            |
|-------------|----------------------------------------|
| **Latencia**| Tiempo de respuesta de la predicciÃ³n   |
| **Throughput**| Cantidad de requests por segundo     |
| **Costo**   | Uso de recursos (CPU, RAM, transferencia) |

> ğŸ” Siempre hay que balancear: mÃ¡s throughput o menos latencia = mÃ¡s costo.

---

## âš–ï¸ OptimizaciÃ³n de recursos

Estrategias para reducir costos:

- **Compartir GPU** entre mÃºltiples modelos.
- Servir varios modelos en el mismo contenedor.
- Optimizar los modelos para inferencia (quantization, pruning).
- Usar modelos especializados para tareas frecuentes.

---

## ğŸ”® Tipos de predicciÃ³n

### Predicciones Batch

- Predicciones en bloque, en momentos programados (ej. cada 4h).
- Datos generalmente histÃ³ricos.
- Permite usar modelos mÃ¡s pesados.

> Ej: generar recomendaciones nocturnas para todos los usuarios.

![[Pasted image 20250605212903.png]]

---

### Predicciones Online

- Respuesta inmediata ante cada request.
- Debe garantizar baja latencia.
- Usa features precomputadas (batch) o en tiempo real (streaming).

> Ej: recomendar productos en una app en tiempo real.

---

### Predicciones con Streaming Features

- Usa features generadas a medida que ocurren eventos.
- Ej: clics recientes, ubicaciÃ³n en vivo.

> âš™ï¸ Requiere infraestructura robusta (ej: Kafka, Pub/Sub).

---

## ğŸ§® ComparaciÃ³n: Batch vs Online

| Aspecto             | Batch (asÃ­ncrono)               | Online (sÃ­ncrono)               |
|---------------------|----------------------------------|---------------------------------|
| Frecuencia          | Programada (cada X horas)        | Tiempo real                     |
| Uso tÃ­pico          | AnÃ¡lisis agregados, scoring masivo| PersonalizaciÃ³n, tiempo real   |
| Requiere cache de features | No                         | A veces sÃ­                      |
| Optimizado para     | Throughput                       | Latencia                        |

---

## â„ï¸ Cold start problem

Problema: no hay informaciÃ³n del usuario nuevo.

- ComÃºn en sistemas de recomendaciÃ³n.
- Soluciones:
  - Usar datos poblacionales (ubicaciÃ³n, edad)
  - Preguntar preferencias iniciales
  - Recomendaciones populares

---

## ğŸ”€ DesafÃ­os del streaming en serving

- Entrenar modelos con features que **no existÃ­an** al momento del entrenamiento.
- Alinear tiempos entre datos de entrada y features.
- Garantizar consistencia entre entornos de entrenamiento e inferencia.

---

## ğŸ”§ Pipeline de predicciones online

1. Request entra con datos del usuario.
2. Se consultan features batch y streaming.
3. Se combinan y se arman inputs del modelo.
4. El modelo devuelve predicciÃ³n.
5. Se registra o actÃºa segÃºn resultado.

---

## âš™ï¸ Beam y pipelines mixtos

Ejemplo: Google Cloud Dataflow / Apache Beam

- Permiten definir pipelines que manejan tanto batch como streaming.
- Ãštiles para feature engineering unificado y consistente.

