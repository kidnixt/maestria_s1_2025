# üß† Elecci√≥n y Ajuste de Modelos en Producci√≥n

## üîÑ Ciclo de Vida de un Proyecto de ML: Rol de la Elecci√≥n del Modelo

 En el ciclo de vida de un proyecto de ML, la **Elecci√≥n del Modelo (ML Model Development)** es una etapa central que interact√∫a con la **Gesti√≥n de Datos (Data Management)**, el **An√°lisis de Negocio (Business Analysis)** y el **Despliegue (Deployment)**. La decisi√≥n sobre qu√© modelo usar impacta directamente en c√≥mo se gestionan los datos, c√≥mo se mide el √©xito de negocio y c√≥mo se pone en producci√≥n.
![[Pasted image 20250717065209.png]]
   
---

## ü§î ¬øC√≥mo elegir un modelo?

Aunque el hype actual est√° en los modelos grandes (redes neuronales profundas, LLMs, etc.), no siempre son la mejor opci√≥n para producci√≥n.
### Ejemplos de tareas y modelos frecuentes:

| Tarea                         | Modelos comunes                                       |
|------------------------------|-------------------------------------------------------|
| Recomendaci√≥n                 | Collaborative Filtering, Matrix Factorization         |
| Clasificaci√≥n                 | √Årboles, Random Forest, Gradient Boosted Trees        |
| An√°lisis de texto             | BERT, GPT, pero tambi√©n Naive Bayes o Logistic Reg.   |
| Detecci√≥n de fraude/anomal√≠as| KNN, Isolation Forest, Clustering, NNs                |

> üìå Consejo: empez√° con algo simple que funcione, despu√©s iter√°s.

---

## üß≠ Enmarcando el Problema: Regresi√≥n vs. Clasificaci√≥n

La forma en que se enmarca el problema define el tipo de modelo a usar. Por ejemplo, la predicci√≥n de precipitaciones puede ser abordada de dos maneras:
* **Regresi√≥n:** Si se predice la cantidad exacta de precipitaci√≥n (ej., 1.7 mm).
* **Clasificaci√≥n multiclase:** Si se predice la probabilidad de que la precipitaci√≥n caiga dentro de rangos espec√≠ficos (ej., 85% de probabilidad de que sea entre 1.5-2.0 mm)

> üîÑ El framing del problema **cambia el tipo de modelo**, la m√©trica, y la arquitectura.

---

## üî† Tipos de clasificaci√≥n

| Tipo              | Descripci√≥n                                           | Ejemplo                                |
|-------------------|-------------------------------------------------------|----------------------------------------|
| Clasificaci√≥n binaria | 2 clases posibles                                 | ¬øSpam o no spam?                       |
| Clasificaci√≥n multiclase | Una clase entre varias posibles              | Tipo de prenda: camiseta, jeans, buzo |
| Clasificaci√≥n multilabel | M√∫ltiples etiquetas por muestra              | Imagen con ‚Äúplaya‚Äù y ‚Äúatardecer‚Äù       |

---

## üß© Factores a considerar al elegir un modelo

- ¬øCu√°ntos datos tengo?
- ¬øCu√°nto c√≥mputo disponible?
- ¬øCu√°nta latencia tolera la aplicaci√≥n?
- ¬øNecesito interpretabilidad?
- ¬øSe puede mantener en producci√≥n?

* **Consideraciones clave:**
    * **Costos de entrenamiento:** En algunos casos, un modelo m√°s complejo puede ser muy costoso de entrenar. 
    * **Latencia de inferencia:** Un modelo m√°s lento en producci√≥n puede impactar negativamente la experiencia del usuario.
    * **Interpretabilidad:** En √°reas como la medicina o las finanzas, entender por qu√© el modelo toma una decisi√≥n es tan importante como la decisi√≥n misma.

> Ejemplo: en un sistema m√©dico se prioriza **interpretabilidad**; en una app de recomendaciones, **latencia**.

---

## ‚úÖ Tips para elegir el modelo

- Evit√° elegir solo por el SOTA (*state-of-the-art*).
- Empez√° simple y escale gradualmente.
- Compar√° modelos bajo las mismas condiciones.
- Consider√°:
  - FP vs FN
  - Accuracy vs recursos
  - Interpretabilidad vs performance
- Entend√© qu√© **supuestos** hace el modelo sobre los datos.

---

## üîÅ Transfer Learning

Usar un modelo preentrenado en una tarea relacionada y ajustarlo a tu problema.  
> Ej: usar BERT entrenado en Wikipedia para clasificar tweets.

---

## üéØ Entrenamiento: Overfitting y Regularizaci√≥n

- El modelo **debe poder overfittear** (memorizar) al dataset antes de poder generalizar.
- Si no puede hacerlo, necesita **m√°s capacidad**.
- Una vez que overfittea, usamos regularizaci√≥n:

### M√©todos de regularizaci√≥n:
- Penalizaci√≥n L1/L2
- Dropout
- Data augmentation
- Batch Normalization

> üéØ Regularizar = evitar que el modelo aprenda ruido

---

## üõë Guardar estado del modelo: Early Stopping

- Cortar el entrenamiento cuando el rendimiento en validaci√≥n deja de mejorar.
- Previene overfitting y ahorra recursos.

---

## üßÆ Par√°metros vs Hiperpar√°metros

| Tipo               | ¬øSe entrena? | Ejemplos                                 |
|--------------------|--------------|------------------------------------------|
| Par√°metros         | ‚úÖ           | Pesos de una red neuronal                |
| Hiperpar√°metros    | ‚ùå (se fijan antes) | Learning rate, capas ocultas, batch size |

---

## üîß Ajuste de Hiperpar√°metros

- Puede ser complejo incluso en modelos chicos.
- Algunos hiperpar√°metros comunes:
  - N√∫mero de capas/neuronas
  - Funci√≥n de activaci√≥n
  - Learning rate
  - Criterios de parada

### M√©todos de ajuste:

| M√©todo          | Descripci√≥n                                     |
|-----------------|-------------------------------------------------|
| Grid search     | Recorre todas las combinaciones posibles        |
| Random search   | Toma combinaciones aleatorias                   |
| Bayesian opt.   | Usa modelos probabil√≠sticos para elegir mejor   |

> üì¶ Frameworks √∫tiles: `Auto-sklearn`, `Optuna`, `Keras Tuner`

---

## üîÅ Ensembles

### ¬øQu√© es?

Combinar **varios modelos** para obtener mejores predicciones.

### Ejemplo:

- Si tres modelos predicen con 70% de precisi√≥n, la combinaci√≥n puede aumentar la certeza global.

### Tipos:

| T√©cnica  | Descripci√≥n                                         |
| -------- | --------------------------------------------------- |
| Bagging  | Entrenar modelos sobre subconjuntos aleatorios      |
| Boosting | Entrenar modelos secuenciales para corregir errores |
| Stacking | Un meta-modelo combina las salidas de otros modelos |

> üìå Ensembles mejoran performance pero:
- Son dif√≠ciles de servir en producci√≥n
- Requieren m√°s c√≥mputo
- Reducen interpretabilidad

---

## ü§ñ AutoML

> ¬øPodemos automatizar todo el pipeline de ML?

- Visi√≥n: **modelo = datos + c√≥mputo**, sin intervenci√≥n humana.
- A√∫n es costoso, pero avanza r√°pido.
- Herramientas: `AutoML`, `AutoKeras`, `TPOT`

---

## üß¨ NAS ‚Äì Neural Architecture Search

- T√©cnica que busca autom√°ticamente la mejor arquitectura para una tarea.
- Define un **espacio de b√∫squeda** y prueba m√∫ltiples configuraciones.

> Ejemplo: elegir cu√°ntas capas, activaciones, conexiones para una red.

---
