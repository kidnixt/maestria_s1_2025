# âœ… Clase 10 â€“ Calidad en Sistemas de ML


---
## ğŸ¯ Objetivo de la clase

Entender cÃ³mo asegurar la **calidad de los sistemas de ML en producciÃ³n**, quÃ© se considera una **falla**, cÃ³mo detectarla, y cÃ³mo mantener un sistema robusto y confiable ante cambios.

---

## ğŸ”§ Tipos de fallas en sistemas ML

> Una falla es cuando **no se cumple una expectativa** respecto a requerimientos.

### 1. Requerimientos funcionales

- Se mide con mÃ©tricas de ML (accuracy, F1, etc).
- Ej: clasificaciÃ³n incorrecta, predicciÃ³n errÃ³nea.

### 2. Requerimientos no funcionales

- Se refiere a la operaciÃ³n del sistema:
  - Latencia
  - Throughput
  - Uptime

---

## âš ï¸ Ejemplos de fallas

| Sistema                 | Falla operacional         | Falla de modelo (funcional)         |
|-------------------------|---------------------------|--------------------------------------|
| Traductor automÃ¡tico    | No responde                | TraducciÃ³n incoherente              |
| BÃºsqueda de imÃ¡genes    | No carga resultados        | Resultados irrelevantes             |
| OCR de documentos       | No escanea                 | ClasificaciÃ³n incorrecta            |
| VehÃ­culo autÃ³nomo       | No enciende                | Falla en evitar colisiÃ³n (grave)    |

> ğŸ§  â€œBuenoâ€ â‰  â€œÃštilâ€ â†’ un modelo con F1=0.98 puede no ser suficiente en producciÃ³n

---

## ğŸ’¥ Fallas frecuentes en producciÃ³n

- **Casos de borde** no contemplados
- Diferencia entre datos de entrenamiento y de producciÃ³n:
  - `Train-serving skew`

![[Pasted image 20250612190529.png]]


  - `Data drift`
  - `Concept drift`
  - `Label drift`
- **Feedback loops degenerativos**

---

## ğŸ“‰ Drift: cambios en los datos

Los modelos operan en ambientes **dinÃ¡micos**, por ende el **ground truth** tambiÃ©n cambia.
â†’ si el modelo queda estÃ¡tico, su rendimiento se degrada (se aleja del ground truth)


![[Pasted image 20250612190903.png]]
### Distribuciones

- `DistribuciÃ³n fuente`: datos con los que entrena el modelo.
- `DistribuciÃ³n objetivo`: datos sobre los que hace inferencia el modelo (producciÃ³n)

---

## ğŸ“Š Tipos de Drift en Sistemas de ML

Cuando los datos cambian con el tiempo, el rendimiento del modelo se puede degradar. Esto se conoce como **drift**, y puede manifestarse de varias formas segÃºn quÃ© parte del sistema cambia.

---

### ğŸŸ  Covariate Drift

En estadÃ­stica, una covariable es una variable que posiblemente predice el resultado bajo estudio. En ML supervisado las features son **covariables.**

**Â¿QuÃ© cambia?**  
- Cambia la distribuciÃ³n fuente (inputs): `P(X)`  
- Pero dado un input, su output no cambia: `P(Y|X)`

**Ejemplo:**  
- En producciÃ³n, llegan mÃ¡s usuarios mayores de 60 aÃ±os que en entrenamiento.
- El modelo aÃºn sabe predecir bien para este grupo, pero como su proporciÃ³n crece, el dataset ya no representa adecuadamente la realidad.
- Predecir P(cancer | paciente)
- P (edad > 60) es mayor en training que en producciÃ³n.
- Pero P(cancer | edad > 60) es lo mismo en training que en producciÃ³n.

![[Pasted image 20250612192254.png]]

#### Causas de Covariate Shift

**Durante Entrenamiento:**
- Sesgos por muestreo:
	- Se aconseja a personas mayores a hacerse mÃ¡s chequeos.
	- Over/undersampling (muchos pacientes viejos de un hospital)

**En ProducciÃ³n:**
- Cambios en el ambiente:
	- P (convertir un usuario gratis a pago | usuario gratuito)
		- Se realiza una campaÃ±a que atrae a usuarios de mayor poder adquisitivo.
		- P (mayor poder adquisitivo) aumenta en los datos.
		- Pero P (convertir un usuario gratis a pago | nivel adquisitivo) se mantiene igual

**Consecuencias:**  
- El modelo puede seguir siendo vÃ¡lido, pero el **peso de cada observaciÃ³n** cambia.
- Puede afectar la representatividad y precisiÃ³n general.

**CÃ³mo detectarlo:**  
- Comparar estadÃ­sticas de features entre entrenamiento y producciÃ³n.
- Tests como KS test, PSI (Population Stability Index).

---

### ğŸ”´ Label Drift

**Â¿QuÃ© cambia?**  
- Cambia la distribuciÃ³n del output: `P(Y)`  
- Pero dado un output, la distribuciÃ³n : `P(X|Y)` se mantiene
- En general Covariate Shift deviene en Label Shift (Shift es lo mismo que drift)

**Ejemplo:**  
- En un hospital, aparece una nueva ola de una enfermedad.
- Aumentan los casos positivos sin cambiar las caracterÃ­sticas de los pacientes.
- Si se inventa una medicina preventiva contra el cÃ¡ncer
	- Se reduce la probabilidad de P (cancer | paciente ) para todas las edades.

**Consecuencias:**  
- Si se usa una muestra aleatoria para evaluaciÃ³n, se verÃ¡ **una caÃ­da de accuracy** por desbalance.
- Puede llevar a decisiones mal calibradas (ej: modelos de riesgo mal evaluados).

**CÃ³mo detectarlo:**  
- Comparar frecuencia de clases entre entrenamiento y producciÃ³n.
- Requiere acceso a ground truth actualizado.

---

### ğŸŸ¡ Data Drift (tambiÃ©n llamado Feature Drift)

**Â¿QuÃ© cambia?**  
- Cambian las estadÃ­sticas de los features `X` (media, varianza, etc.) O sea, propiedades estadÃ­sticas de las features cambian. **El modelo entrenado ya no es relevante una vez que los datos cambiaron** 

**Â¿QuÃ© no necesariamente cambia?**  
- No implica directamente un cambio en `P(Y|X)`

**Ejemplo:**  
- Cambia el paÃ­s de origen de los usuarios.
- La edad promedio sube de 30 a 45 aÃ±os.

![[Pasted image 20250612193110.png]]


**Consecuencias:**  
- Puede indicar **desalineaciÃ³n entre datasets**.
- TambiÃ©n puede anticipar covariate o concept drift.

**CÃ³mo detectarlo:**  
- Medir media, varianza, outliers, histograma de features.
- TÃ©cnicas de detecciÃ³n no supervisadas.

---

### ğŸ”µ Concept Drift

**Â¿QuÃ© cambia?**  
- Cambia la relaciÃ³n entre input y output: `P(Y|X)`  
- Se puede mantener la distribuciÃ³n de inputs `P(X)`
- RelaciÃ³n entre las features y las etiquetas cambia.
- El significado en sÃ­ de lo que estamos intentando predecir cambia.

![[Pasted image 20250612193603.png]]

![[Pasted image 20250612193618.png]]

**Ejemplo:**  
- Antes, comprar pijamas durante el dÃ­a era tÃ­pico de personas desempleadas.  
- En pandemia, todos lo hacen: el concepto de â€œpersona con disponibilidad diurnaâ€ cambia.

**Consecuencias:**  
- **Grave degradaciÃ³n de performance**
- Los supuestos aprendidos ya no se sostienen.

**CÃ³mo detectarlo:**  
- Solo puede detectarse con **ground truth actualizado**.
- Monitorear mÃ©tricas de desempeÃ±o en producciÃ³n (accuracy, F1, etc.)

---

### ğŸ“˜ ComparaciÃ³n General

| Tipo            | Cambia...         | Se mantiene... | Requiere etiquetas para detectar |
| --------------- | ----------------- | -------------- | -------------------------------- |
| Covariate Drift | `P(X)`            | `P(Y\X)`       | âŒ (puede ser no supervisado)     |
| Label Drift     | `P(Y)`            | `P(X\Y)        | âœ…                                |
| Data Drift      | EstadÃ­sticas de X | -              | âŒ                                |
| Concept Drift   | `P(Y\X)`          | `P(X)`         | âœ…                                |

---

### ğŸ§  Observaciones clave

- **Concept drift** puede ser:
  - *Repentino* (un cambio brusco en la relaciÃ³n)
  - *Gradual* (cambia lentamente con el tiempo)
  - *Estacional* (se repite cÃ­clicamente)

- **Covariate drift** puede parecer inofensivo pero puede causar **bias** en el modelo.

- **Label drift** suele indicar un cambio en el contexto o distribuciÃ³n real del problema.

---

---

## ğŸ§ª CÃ³mo detectar drift

- **Observabilidad**: inputs, outputs y mÃ©tricas deben ser logueados
- Las **requests de predicciones** deben ser logueadas.
	- O por lo menos loguear los datos de la request
	- Analizar estos datos es clave para detectar el drift que cause el deterioro de los modelos. 
- Loguear tambiÃ©n el ground truth asociado.
	- AsÃ­ luego se puede usar para entrenar.
- Analizar estadÃ­sticamente:
  - Cambios en distribuciones
  - Shift en predicciones
  - Comparar ground truth vs predicho
- VisualizaciÃ³n con dashboards

### Herramientas

- `TF Data Validation`
- `Scikit-multiflow`
- `WhyLabs`
- Herramientas nativas (GCP, AWS, Azure)

---

## ğŸ” RetroalimentaciÃ³n degenerativa (degenerate feedback loop)

> Las **predicciones influyen los datos futuros**  
> (el modelo afecta el mundo que luego lo alimenta)

### Ejemplo: recomendadores

- RecomendÃ¡s un Ã­tem
- Usuario lo ve â†’ se registra como "buena predicciÃ³n"
- El sistema refuerza ese Ã­tem â†’ **homogeneizaciÃ³n**

---

## ğŸ› ï¸ CÃ³mo detectar loops degenerativos

- Medir popularidad media de los Ã­tems recomendados
- % de Ã­tems de la long-tail (menos populares)
- Comparar `hit-rate` vs popularidad

ğŸ“„ Chia et al. (2021) â€“ *RecList: Behavioral Testing of Recommenders*  
https://arxiv.org/abs/2111.09963

---

## ğŸ›¡ï¸ CÃ³mo prevenir loops degenerativos

- **Aleatoriedad controlada**
  - Ej: TikTok muestra videos nuevos a grupos aleatorios al principio

- **Features posicionales**
  - PosiciÃ³n en el feed: no es lo mismo estar 1Âº que 5Âº

---

## ğŸ§¬ Otros cambios en datos

- Cambios en las features (se agregan o eliminan)
- Cambios en las etiquetas (p.â€¯ej. agregar clase â€œneutralâ€ a anÃ¡lisis de sentimiento)

### Estrategias

- **Schema bridge**
- **ImputaciÃ³n**: media, mediana, moda o clase mÃ¡s frecuente

---

## ğŸ” Por quÃ© monitorear

- Los datos de entrenamiento son **una foto de la realidad**
- El entorno **cambia constantemente**, los modelos no mejoran con el tiempo.
- Queremos:
  - Detectar degradaciÃ³n
  - Alertar rÃ¡pido
  - Tomar decisiones de reentrenamiento

---

## ğŸ“Š QuÃ© monitorear

| Elemento          | Ejemplos                                           |
|-------------------|----------------------------------------------------|
| Accuracy           | MÃ©tricas offline vs online                         |
| Predicciones       | DistribuciÃ³n de clases, confianza promedio         |
| Features           | Rangos, outliers, frecuencia por segmento          |
| Sistema            | Latencia, throughput, uptime                       |

---

## ğŸ“º Monitoreo

- **Datos cambiantes:**
	- Los datos de entrenamiento son tan viejos que ya no representan la realidad.
- **Obsolencia de los modelos:**
	- El ambiente cambia
	- El comportamiento de los usuarios cambia
	- Escenarios adversarios.
- **RetroalimentaciÃ³n negativa de los modelos**

---

## ğŸ§© Monitoreo de input

- Â¿Los inputs son vÃ¡lidos? (tipos, rangos)
- Â¿Hay cambios significativos en la distribuciÃ³n con la historica?
- Â¿Cambios en segmentos clave? Como se ve la evoluciÃ³n a nivel de segmentos

---

## ğŸ§ª Monitoreo de predicciones

- Comparar distribuciÃ³n de predicciones actuales vs histÃ³ricas
- Si se tiene ground truth: calcular accuracy, F1, etc.

---

## ğŸ–¥ï¸ Monitoreo funcional vs no funcional

| Tipo                     | Incluye                                       |
|--------------------------|-----------------------------------------------|
| Funcional (ML)           | Accuracy, cambios en datos de serving         |
| No funcional (sistema)   | Latencia, throughput, uptime                  |

![[Pasted image 20250612195356.png]]

---

## ğŸ§° Herramientas y sugerencias

- Usar `logging` de plataformas:
  - `Google Cloud Ops`, `AWS CloudWatch`, `Azure Monitor`
- Centralizar logs en un solo sistema
- Crear alertas basadas en logs
- Visualizar mÃ©tricas relevantes (Dashboards)

---

## ğŸ“‹ Â¿QuÃ© loguear?

- Inputs y predicciones
- Ground truth (si estÃ¡ disponible)
- Errores del sistema
- Casos especiales o anÃ³malos

> Logging sirve tambiÃ©n para crear datasets futuros para reentrenar


![[Pasted image 20250612200942.png]]
---

## ğŸ”„ Â¿QuÃ© hacer cuando se detecta drift?

1. **Determinar quÃ© datos aÃºn son vÃ¡lidos**
   - Filtrar temporalmente (ej: Ãºltimos 30 dÃ­as)
   - Descartar los malos y agregar nuevos datos.
   - Eliminar segmentos irrelevantes.
   - Crear un nuevo dataset.

2. **Reentrenamiento**
   - Fine-tune del modelo existente
   - Entrenamiento desde cero con nuevos datos

---

## ğŸ” PolÃ­ticas de reentrenamiento

| Tipo       | CuÃ¡ndo se activa                                                                                                    |
| ---------- | ------------------------------------------------------------------------------------------------------------------- |
| Manual     | Bajo demanda, anÃ¡lisis humano, cuando hay suficientes datos nuevos.                                                 |
| AutomÃ¡tico | - PeriÃ³dico (ej: semanal, mensual)<br>- Cuando se detecta que el rendimiento baja<br>- Cuando se detecta Data Drift |
![[Pasted image 20250612201346.png]]

![[Pasted image 20250612201406.png]]

![[Pasted image 20250612201421.png]]
---

## ğŸ§ª EvaluaciÃ³n en producciÃ³n

> Siempre evaluar **offline primero**

### Estrategias

- **Shadow deployment**: se predice en paralelo, sin afectar producciÃ³n
- **Canary release**: se expone a un % pequeÃ±o de usuarios
- **A/B testing**: comparar dos versiones del modelo
- **Bandits**: balanceo dinÃ¡mico segÃºn performance
